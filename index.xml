<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Jae&#39;s Blog</title>
    <link>https://shinjaehyeok.github.io/</link>
      <atom:link href="https://shinjaehyeok.github.io/index.xml" rel="self" type="application/rss+xml" />
    <description>Jae&#39;s Blog</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Copyright 2022 Jaehyeok Shin. All rights reserved.</copyright><lastBuildDate>Sun, 05 Jun 2022 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://shinjaehyeok.github.io/images/icon_huc50b28ae5ce84fcc767e7691e9c0c0ae_20181_512x512_fill_lanczos_center_3.png</url>
      <title>Jae&#39;s Blog</title>
      <link>https://shinjaehyeok.github.io/</link>
    </image>
    
    <item>
      <title>Advantages of Sequential Hypothesis Testing: 2. Flexibility and Safety</title>
      <link>https://shinjaehyeok.github.io/post/statistics/sequential_test_safety/sequential_test_flexibility_safety/</link>
      <pubDate>Sun, 05 Jun 2022 00:00:00 +0000</pubDate>
      <guid>https://shinjaehyeok.github.io/post/statistics/sequential_test_safety/sequential_test_flexibility_safety/</guid>
      <description>
&lt;script src=&#34;https://shinjaehyeok.github.io/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;In this follow-up post, we explain another advantage of sequential hypothesis testing: Flexibility and safety. Please visit the previous &lt;a href=&#34;https://shinjaehyeok.github.io/post/statistics/statistical_test_1/stcd-tutorial/&#34;&gt;post&lt;/a&gt; for an introduction to Wald’s sequential probability ratio test (&lt;a href=&#34;https://en.wikipedia.org/wiki/Sequential_probability_ratio_test&#34;&gt;SPRT&lt;/a&gt;) and a discussion of its sample efficiency.&lt;/p&gt;
&lt;div id=&#34;can-we-early-stop-the-testing-if-the-p-value-already-reached-below-alpha&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Can we early stop the testing if the p-value already reached below &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;?&lt;/h2&gt;
&lt;p&gt;Again, let’s start with a simple coin toss example where we observe a sequence of independent observations &lt;span class=&#34;math inline&#34;&gt;\(X_1, X_2, \dots \in \{0,1\}\)&lt;/span&gt; from a Bernoulli distribution &lt;span class=&#34;math inline&#34;&gt;\(B(p)\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(p \in (0,1)\)&lt;/span&gt;. We are interested in testing whether the underlying coin is pair (&lt;span class=&#34;math inline&#34;&gt;\(H_0: p = 0.5\)&lt;/span&gt;) or biased toward to head (&lt;span class=&#34;math inline&#34;&gt;\(H_1: p &amp;gt; 0.5\)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;From the previous &lt;a href=&#34;https://shinjaehyeok.github.io/post/statistics/statistical_test_1/stcd-tutorial/&#34;&gt;post&lt;/a&gt;, we know that the binomial test is the best fixed sample size test. Formally, let’s set up the binomial test of level &lt;span class=&#34;math inline&#34;&gt;\(\alpha = 0.05\)&lt;/span&gt; with the minimum power of &lt;span class=&#34;math inline&#34;&gt;\(\beta = 0.95\)&lt;/span&gt; at &lt;span class=&#34;math inline&#34;&gt;\(p = 0.6\)&lt;/span&gt;. In this case, the minimum sample size is given by the following R script.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1)
printf &amp;lt;- function(...) invisible(print(sprintf(...)))
n_to_power &amp;lt;- function(n, p0, p1, alpha) {
  n &amp;lt;- ceiling(n)
  thres &amp;lt;- qbinom(alpha, n, p0, lower.tail = FALSE) # Reject null if s &amp;gt; thres
  pwr &amp;lt;- pbinom(thres, n, p1, lower.tail = FALSE)
  return(pwr)
}

power_to_n &amp;lt;- function(beta, p0, p1, alpha) {
  f &amp;lt;- function(n) {
    beta - n_to_power(n, p0, p1, alpha)
  }
  root &amp;lt;- stats::uniroot(f, c(1, 1e+4), tol = 1e-6)
  return(ceiling(root$root))
}

alpha &amp;lt;- 0.05
beta &amp;lt;- 0.95
p0 &amp;lt;- 0.5
p1 &amp;lt;- 0.6
n &amp;lt;- power_to_n(beta, p0 , p1 , alpha)
printf(&amp;quot;%i is the minimum sample size to achieve test level %.2f and power %.2f at p= %.1f against the null p=%.2f.&amp;quot;, 
       n, alpha, beta, p1, p0)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;280 is the minimum sample size to achieve test level 0.05 and power 0.95 at p= 0.6 against the null p=0.50.&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the standard fixed sample size setting, we compute the p-value once collecting all 280 samples. Then, if the p-value is less than the test level &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;, we reject the null hypothesis. In this case, we can control the type-1 error below &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; and achieve the minimum power &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; under the alternative (&lt;span class=&#34;math inline&#34;&gt;\(p = 0.6\)&lt;/span&gt;), as the following simulation demonstrates.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;binom_p_value &amp;lt;- function(s, n, p0) {
   return(pbinom(s-1, n, p0, lower.tail = FALSE))
}
run_binom_simul &amp;lt;- function(p_true, n, p0, alpha, max_iter) {
  # We shorten the simulation by using the fact X_1 + ...+X_n ~ Binom(n, p)
  s_vec &amp;lt;- rbinom(max_iter, size = n, prob = p_true)
  p_vec &amp;lt;- sapply(s_vec, binom_p_value, n = n, p0 = p0)
  return(mean(p_vec &amp;lt;= alpha))
}
# Under the null
max_iter &amp;lt;- 1e+4L
type_1_err &amp;lt;- run_binom_simul(p_true = p0, n, p0, alpha, max_iter)
printf(&amp;quot;Type-1 error of the binomial test is %.2f.&amp;quot;, type_1_err)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Type-1 error of the binomial test is 0.04.&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Under the alternative
pwr &amp;lt;- run_binom_simul(p_true = p1, n, p0, alpha, max_iter)
printf(&amp;quot;Power of the binomial test at p=%.1f is %.2f.&amp;quot;, p1, pwr)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Power of the binomial test at p=0.6 is 0.95.&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What if we observe the coin toss one by one and we compute the p-value at each time whenever a new coin toss happens? In the previous &lt;a href=&#34;https://shinjaehyeok.github.io/post/statistics/statistical_test_1/stcd-tutorial/&#34;&gt;post&lt;/a&gt;, we noticed that Wald’s &lt;a href=&#34;https://en.wikipedia.org/wiki/Sequential_probability_ratio_test&#34;&gt;sequential probability ratio test (SPRT)&lt;/a&gt; can achieve a high sample efficiency by adaptively stopping the test procedure. Why not do the same trick to the binomial test? Can we early stop the testing if the p-value already reached below &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;? Let’s run a simulation to check whether this early stopping still results in a type-1 error below the test level &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;run_binom_early_stop &amp;lt;- function(p_true, n, p0, alpha) {
  s &amp;lt;- 0
  for (i in 1:n) {
    s &amp;lt;- s + rbinom(1, size = 1, prob = p_true)
    p &amp;lt;-  binom_p_value(s, i, p0)
    if (p &amp;lt;= alpha) {
      return(TRUE)
    }
  }
  return(FALSE)
}
# Under the null
early_stopp_type_1_err &amp;lt;- mean(replicate(max_iter,{
  run_binom_early_stop(p_true = 0.5, n, p0, alpha)
  }))
printf(&amp;quot;Type-1 error of the early stopped binomial test is %.2f.&amp;quot;, early_stopp_type_1_err)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Type-1 error of the early stopped binomial test is 0.26.&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It turns out that the early stopping strategy increased the type 1 error significantly above the target level. This inflation of type-1 error is an example of &lt;a href=&#34;https://en.wikipedia.org/wiki/Data_dredging&#34;&gt;p-hacking&lt;/a&gt;. This issue is getting worse if we run a longer test.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(4)
s_vec &amp;lt;- cumsum(rbinom(n, size = 1, prob = 0.5))
f &amp;lt;- function(i) binom_p_value(s_vec[i], i, p0)
p_vec &amp;lt;- sapply(seq_along(s_vec), f)
plot(seq_along(p_vec), p_vec, type = &amp;quot;l&amp;quot;, xlab= &amp;quot;n&amp;quot;, ylab =&amp;quot;p-value&amp;quot;)
abline(h = alpha, col = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://shinjaehyeok.github.io/post/statistics/sequential_test_safety/2.statistical_test_files/figure-html/binom_p_value_path-1.png&#34; width=&#34;672&#34; /&gt;
&lt;strong&gt;Fig. 1 An example sample path of p-values hitting the threshold &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; although the underlying samples generated under the null. The red horizontal line corresponds to the test level &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;. &lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;can-we-continue-to-collect-more-samples-if-the-p-value-at-the-end-is-slightly-above-alpha&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Can we continue to collect more samples if the p-value at the end is slightly above &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;?&lt;/h2&gt;
&lt;p&gt;Okay, we learned that we should not early stop and wait to collect all samples of the pre-calculated size. But what if the p-value is slightly above &lt;span class=&#34;math inline&#34;&gt;\(\alpha = 0.05\)&lt;/span&gt; - let’s say &lt;span class=&#34;math inline&#34;&gt;\(p = 0.15\)&lt;/span&gt;. Maybe, if we would have collected a bit more samples then the p-value might possibly be less than &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; and we could reject the null. Can we continue to collect more samples to see whether the p-value becomes less than &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; or goes away from it?&lt;/p&gt;
&lt;p&gt;Let’s run a simple simulation to answer the question. For each run, we first collect the minimum sample size &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; computed to achieve the minimum power &lt;span class=&#34;math inline&#34;&gt;\(\beta = 0.95\)&lt;/span&gt;. Then, compute the p-value based on the collected &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; samples. If the p-value is less than &lt;span class=&#34;math inline&#34;&gt;\(0.2\)&lt;/span&gt; then we collect another &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; samples and re-compute the p-value based on &lt;span class=&#34;math inline&#34;&gt;\(2n\)&lt;/span&gt; samples. Finally, we reject the null if the final p-value is less than or equal to the test level &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;run_binom_a_bit_more_simul &amp;lt;- function(p_true, n, p0, alpha, max_iter) {
  # We shorten the simulation by using the fact X_1 + ...+X_n ~ Binom(n, p)
  s_vec &amp;lt;- rbinom(max_iter, size = n, prob = p_true)
  p_vec &amp;lt;- sapply(s_vec, binom_p_value, n = n, p0 = p0)
  
  # If p-value &amp;gt; alpha and &amp;lt; 0.2, collect n more samples
  above_alpha_ind &amp;lt;- which(p_vec &amp;gt; alpha &amp;amp; p_vec &amp;lt; 0.2)
  s_vec[above_alpha_ind] &amp;lt;- 
    s_vec[above_alpha_ind] + rbinom(length(above_alpha_ind), size = n, prob = p_true)
  p_vec[above_alpha_ind] &amp;lt;- sapply(s_vec[above_alpha_ind], binom_p_value, n = 2*n, p0 = p0)
  return(mean(p_vec &amp;lt;= alpha))
}
a_bit_more_type_1_err &amp;lt;- run_binom_a_bit_more_simul(p_true = p0, n, p0, alpha, max_iter)
printf(&amp;quot;Type-1 error of the binomial test with the contional continuation is %.3f.&amp;quot;, a_bit_more_type_1_err)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Type-1 error of the binomial test with the contional continuation is 0.063.&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this case, the type-1 error increased above the test level &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; though it is not as dramatic as the early stopping case. If we continuously collect and compute p-values rather than collect a single batch then the inflation of the type-1 error becomes more significant. This type of continuation of the testing procedure is also an example of &lt;a href=&#34;https://en.wikipedia.org/wiki/Data_dredging&#34;&gt;p-hacking&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;run_binom_a_bit_more2_run &amp;lt;- function(p_true, n, p0, alpha) {
  s &amp;lt;- rbinom(1, size = n, prob = p_true)
  p &amp;lt;-  binom_p_value(s, n, p0)
  if (p &amp;lt;= alpha) {
    # If first n samples give p-value less than alpha
    # Reject the null
    return(TRUE)
  } else if (p &amp;gt;= 0.2) {
    # If first n samples give p-value above 0.2
    # Fail to reject the null and stop the test
    return(FALSE)
  }
  # Otherwise, continue the test until we collect another n samples.
  for (i in 1:n) {
    s &amp;lt;- s + rbinom(1, size = 1, prob = p_true)
    p &amp;lt;-  binom_p_value(s, i, p0)
    if (p &amp;lt;= alpha) {
      return(TRUE)
    }
  }
  return(FALSE)
}
# Under the null
a_bit_more2_type_1_err &amp;lt;- mean(replicate(max_iter,{
  run_binom_a_bit_more2_run(p_true = 0.5, n, p0, alpha)
  }))
printf(&amp;quot;Type-1 error of the binomial test with the additional continous monitoring is %.2f.&amp;quot;, a_bit_more2_type_1_err)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Type-1 error of the binomial test with the additional continous monitoring is 0.18.&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;always-valid-p-values---flexibly-and-safely-early-stop-or-continue-the-test.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Always-valid p-values - flexibly and safely early stop or continue the test.&lt;/h2&gt;
&lt;p&gt;So far, we have observed how the early stop or continual test results in the type-1 error inflation for fixed sample size tests. The root cause of this inflation is that the p-value from a fixed sample size test is valid only at the pre-specified fixed sample size. In other words, let &lt;span class=&#34;math inline&#34;&gt;\(p_n\)&lt;/span&gt; be the p-value of &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; samples. Then we have
&lt;span class=&#34;math display&#34;&gt;\[
P_{H_0}\left(p_n \leq \alpha\right) \leq \alpha, ~~\forall n.
\]&lt;/span&gt;
However, to early stop or continue the test without the type-1 error inflation, we need the following stronger inequality.
&lt;span class=&#34;math display&#34;&gt;\[
P_{H_0}\left(\exists n: p_n \leq \alpha\right) \leq \alpha.
\]&lt;/span&gt;
Note that fixed sample based p-values do not necessarily satisfy the above strong inequality.&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;If a random sequence &lt;span class=&#34;math inline&#34;&gt;\(\{p_n\}_{n \geq 1}\)&lt;/span&gt; satisfies the above stronger inequality then it is called “always-valid p-value”. Every sequential hypothesis test has the corresponding always-valid p-value. Therefore, we can flexibly and safely early stop or continue the test.&lt;/p&gt;
&lt;p&gt;As a concrete example, let’s compute the always-valid p-value of the Wald’s SPRT. Recall that for our Bernoulli example with &lt;span class=&#34;math inline&#34;&gt;\(H_0: p = p_0\)&lt;/span&gt; vs &lt;span class=&#34;math inline&#34;&gt;\(H_1: p = p_1\)&lt;/span&gt;, the Wald’s SPRT is a sequential hypothesis testing procedures defined as follows.
* Set &lt;span class=&#34;math inline&#34;&gt;\(S_0 := 0\)&lt;/span&gt; and for each observation &lt;span class=&#34;math inline&#34;&gt;\(X_n\)&lt;/span&gt;, compute the probability ratio &lt;span class=&#34;math inline&#34;&gt;\(\Lambda_n\)&lt;/span&gt; by
&lt;span class=&#34;math display&#34;&gt;\[
\Lambda_n := \left(\frac{p_1}{p_0}\right)^{X_n}\left(\frac{1-p_1}{1-p_0}\right)^{1-X_n}.
\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Update &lt;span class=&#34;math inline&#34;&gt;\(S_n := S_{n-1} + \log \Lambda_n\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Make one of three following decisions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(S_n \geq b\)&lt;/span&gt; then stop and reject the null (since there is only one alternative, it is equivalent to accept the alternative).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(S_n \leq a\)&lt;/span&gt; then stop and reject the alternative (or accept the null).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Otherwise, continue to the next iteration.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here, we can set &lt;span class=&#34;math inline&#34;&gt;\(a = \log(1-\beta)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(b = \log\frac{1}{\alpha}\)&lt;/span&gt;. In this case, we can construct an always-valid p-value by
&lt;span class=&#34;math display&#34;&gt;\[
p_n := \min\left\{1, e^{-S_n}\right\}.
\]&lt;/span&gt;
We can easily check &lt;span class=&#34;math inline&#34;&gt;\(p_n \leq \alpha \Leftrightarrow S_n \geq b = \log(1/\alpha)\)&lt;/span&gt;. Therefore, in terms of the rejection of the null, tracking &lt;span class=&#34;math inline&#34;&gt;\(S_n\)&lt;/span&gt; with the threshold &lt;span class=&#34;math inline&#34;&gt;\(\log(1/\alpha)\)&lt;/span&gt; of the Wald’s SPRT is equivalent to tracking &lt;span class=&#34;math inline&#34;&gt;\(p_n\)&lt;/span&gt; with the threshold &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;. From the type-1 error control of the Wald’s SPRT, we can check &lt;span class=&#34;math inline&#34;&gt;\(p_n\)&lt;/span&gt; is an always-valid p-value satisfying the above stronger inequality.&lt;/p&gt;
&lt;p&gt;As a remark, the always-valid p-value above is not uniformly distributed. In fact, we can prove that &lt;span class=&#34;math inline&#34;&gt;\(p_n \rightarrow 1\)&lt;/span&gt; almost surely under the null and &lt;span class=&#34;math inline&#34;&gt;\(p_n \rightarrow 0\)&lt;/span&gt; almost surely under the alternative as &lt;span class=&#34;math inline&#34;&gt;\(n \to \infty\)&lt;/span&gt;. As it is not uniformly distributed, the standard interpretation of the strength of evidence in terms of p-value is not applicable. In fact, though tracking &lt;span class=&#34;math inline&#34;&gt;\(S_n\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(p_n\)&lt;/span&gt; are equivalent, it is recommended to track &lt;span class=&#34;math inline&#34;&gt;\(S_n\)&lt;/span&gt; directly as &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{E}S_n = \mathbb{E}[N] \mathrm{KL}(p_1||p_0)\)&lt;/span&gt; is the information gain under the alternative.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1)
sprt_p_value &amp;lt;- function(p_true, 
                         p0 = 0.5,
                         p1 = 0.6,
                         max_iter = 1e+3L){
  s &amp;lt;- 0
  # Set a placeholder only for visualization.
  # We let the p-value path reaches the maximum to see the convergence
  p_val_history &amp;lt;- rep(NA, max_iter) 
  for (n in 1:max_iter) {
    # Observe a new sample
    x &amp;lt;- rbinom(1, 1, p_true)
    # Update S_n
    s &amp;lt;- s + ifelse(x == 1, log(p1/p0), log((1-p1)/(1-p0)))
    p_val_history[n] &amp;lt;- min(c(1,exp(-s)))
  }
  return(p_val_history)
}

# Under the null
null_p_val &amp;lt;- sprt_p_value(p_true = 0.5)
plot(seq_along(null_p_val), null_p_val, type = &amp;quot;l&amp;quot;,
     xlab = &amp;quot;n&amp;quot;, ylab = &amp;#39;p-value&amp;#39;,
     ylim = c(0, 1),
     main = &amp;quot;Under the null&amp;quot;)
abline(h = alpha, col = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://shinjaehyeok.github.io/post/statistics/sequential_test_safety/2.statistical_test_files/figure-html/sprt_p_value_path_null-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Under the alternative
alter_p_val &amp;lt;- sprt_p_value(p_true = 0.6)
plot(seq_along(alter_p_val), alter_p_val, type = &amp;quot;l&amp;quot;,
     xlab = &amp;quot;n&amp;quot;, ylab = &amp;#39;p-value&amp;#39;,
     ylim = c(0, 1),
     main = &amp;quot;Under the alternative&amp;quot;)
abline(h = alpha, col = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://shinjaehyeok.github.io/post/statistics/sequential_test_safety/2.statistical_test_files/figure-html/sprt_p_value_path_null-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;Fixed sample size tests can suffer from type-1 error inflation if the test is not stopped at the pre-specified time. In contrast, sequential hypothesis tests have great flexibility that allows researchers to early stop or continue the experiment without inflating type-1 error.&lt;/p&gt;
&lt;!--
## 1. Fixed sample size

In the classical setting like scientific researches or survey analyses, it is often expensive to collect observations. Therefore, the sample size is often determined by external factors such as research budget or the number of available respondents at the survey time rather than the power analysis result. Even sometimes, a pre-collected set of samples is given to analysts and analysts themselves have no or very small control on the collected sample size. This is a reason why, in many textbook scenarios like above, samples of a fixed number are given and we are asked to perform fixed sample size tests. 


## 2. Sequential observations
In industrial settings, data are often collected by sensors or logs from which we can access a stream of observations in real time. 

 This is a classical example of statistical hypothesis testing and there are various standard ways to perform a test including Z-test (asymptotic test for a large sample size) and binomial test (exact test for small-to-medium sample  sizes). --&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;In fact, in most standard fixed sample tests, we have &lt;span class=&#34;math inline&#34;&gt;\(P_{H_0}\left(\exists n: p_n \leq \alpha\right) = 1\)&lt;/span&gt;.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Advantages of Sequential Hypothesis Testing: 1. Sample efficiency</title>
      <link>https://shinjaehyeok.github.io/post/statistics/sequential_test_efficiency/sequential_test_sample_efficiency/</link>
      <pubDate>Fri, 03 Jun 2022 00:00:00 +0000</pubDate>
      <guid>https://shinjaehyeok.github.io/post/statistics/sequential_test_efficiency/sequential_test_sample_efficiency/</guid>
      <description>
&lt;script src=&#34;https://shinjaehyeok.github.io/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;In this and a follow-up posts, we explain two main advantages of sequential hypothesis testing methods compared to standard tests based on fixed sample size.&lt;/p&gt;
&lt;div id=&#34;sample-efficiency-in-practice&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Sample efficiency in practice&lt;/h2&gt;
&lt;p&gt;As a working example, suppose we have observed a sequence of independent coin tosses, which are encoded by &lt;span class=&#34;math inline&#34;&gt;\(X_n = 1\)&lt;/span&gt; if the &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;-th observation is head and &lt;span class=&#34;math inline&#34;&gt;\(X_n = 0\)&lt;/span&gt; if it is tail. Statistically, we can model this sequence by independently and identically distributed (i.i.d.) random observations &lt;span class=&#34;math inline&#34;&gt;\(X_1, X_2, \dots \in \{0,1\}\)&lt;/span&gt; from a Bernoulli distribution &lt;span class=&#34;math inline&#34;&gt;\(B(p)\)&lt;/span&gt; with unknown success probability &lt;span class=&#34;math inline&#34;&gt;\(p \in (0,1)\)&lt;/span&gt;. How can we test whether the coin is fair, i.e., &lt;span class=&#34;math inline&#34;&gt;\(p = p_0:= 0.5\)&lt;/span&gt; or not. For a simple presentation, we assume that if the coin is biased then the biased success probability &lt;span class=&#34;math inline&#34;&gt;\(p_1\)&lt;/span&gt; must be larger than &lt;span class=&#34;math inline&#34;&gt;\(p_0\)&lt;/span&gt;.&lt;/p&gt;
&lt;div id=&#34;a-difficulties-in-sample-size-calculation-in-fixed-sample-size-tests&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;a) Difficulties in sample size calculation in fixed sample size tests&lt;/h3&gt;
&lt;p&gt;If we know the success probability &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; must be equal to a constant &lt;span class=&#34;math inline&#34;&gt;\(p_1 &amp;gt; 0.5\)&lt;/span&gt; when the coin turns out to be unfair or biased then the &lt;a href=&#34;https://en.wikipedia.org/wiki/Neyman%E2%80%93Pearson_lemma&#34;&gt;Neyman-Pearson lemma&lt;/a&gt; shows that for a fixed test level &lt;span class=&#34;math inline&#34;&gt;\(\alpha \in (0,1)\)&lt;/span&gt;, the most powerful test is the likelihood-ratio test (LRT) rejecting the null (&lt;span class=&#34;math inline&#34;&gt;\(H_0: p = p_0\)&lt;/span&gt;) in a favor of the alternative (&lt;span class=&#34;math inline&#34;&gt;\(H_1: p = p_1\)&lt;/span&gt;) if &lt;span class=&#34;math inline&#34;&gt;\(\bar{X}_n \geq c_\alpha\)&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(\bar{X}_n := \sum_{i=1}^n X_i /n\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(c_\alpha\)&lt;/span&gt; is a positive constant satisfying &lt;span class=&#34;math inline&#34;&gt;\(P_{H_0}(\bar{X}_n \geq c_\alpha) = \alpha\)&lt;/span&gt;.&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;In fact, LRT is a uniformly most powerful test against all possible alternatives &lt;span class=&#34;math inline&#34;&gt;\(p_1\)&lt;/span&gt; larger than &lt;span class=&#34;math inline&#34;&gt;\(p_0\)&lt;/span&gt;. Hence, if the sample size and test level are fixed, checking whether the sample mean is larger than the critical value &lt;span class=&#34;math inline&#34;&gt;\(c_\alpha\)&lt;/span&gt; is all you need to do. This test is also called the exact &lt;a href=&#34;https://en.wikipedia.org/wiki/Binomial_test&#34;&gt;Binomial test&lt;/a&gt;.&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Example R script&lt;/em&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1)
printf &amp;lt;- function(...) invisible(print(sprintf(...)))
n &amp;lt;- 100L

# Testing fair coin sample (alpha = 0.05)
fair_coin_sample &amp;lt;- rbinom(n, size = 1, prob = 0.5) 
fair_coin_test_result &amp;lt;- binom.test(sum(fair_coin_sample), n, p = 0.5, alternative = &amp;quot;greater&amp;quot;)
printf(&amp;quot;p-value of a fair coin test is %.3f&amp;quot;, fair_coin_test_result$p.value)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;p-value of a fair coin test is 0.691&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Testing biased coin sample (alpha = 0.05)
biased_coin_sample &amp;lt;- rbinom(n, size = 1, prob = 0.6)
biased_coin_test_result &amp;lt;- binom.test(sum(biased_coin_sample), n, p = 0.5, alternative = &amp;quot;greater&amp;quot;)
printf(&amp;quot;p-value of a biased coin test is %.3f&amp;quot;, biased_coin_test_result$p.value)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;p-value of a biased coin test is 0.028&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;However, in practice, we rarely know what the alternative success rate could be before an experiment. Instead, we set a minimum effect size of interest &lt;span class=&#34;math inline&#34;&gt;\(\Delta &amp;gt; 0\)&lt;/span&gt; that we practically consider as a meaningful bias from &lt;span class=&#34;math inline&#34;&gt;\(p_0\)&lt;/span&gt;. Based on the minimum effect size &lt;span class=&#34;math inline&#34;&gt;\(\Delta\)&lt;/span&gt; and desired minimum power &lt;span class=&#34;math inline&#34;&gt;\(\beta \in (0,1)\)&lt;/span&gt;, we can compute the minimum sample size we need to perform a statistical test that can detect &lt;span class=&#34;math inline&#34;&gt;\(p_1 &amp;gt;= p_0 + \Delta\)&lt;/span&gt; with probability at least &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; if the coin is actually biased.&lt;/p&gt;
&lt;p&gt;The size of &lt;span class=&#34;math inline&#34;&gt;\(\Delta\)&lt;/span&gt; depends on the application. For example, If we just play a game at a party with friends, we may still consider a coin with a success probability up to &lt;span class=&#34;math inline&#34;&gt;\(0.51\)&lt;/span&gt; as a “fairly fair” coin we can use. In this case, the minimum effect size is &lt;span class=&#34;math inline&#34;&gt;\(\Delta = 0.1\)&lt;/span&gt;. On the other hand, if we play a serious gamble with a large bet on the coin toss then even &lt;span class=&#34;math inline&#34;&gt;\(0.501\)&lt;/span&gt; can be viewed as an unfair coin and the minimum effect size &lt;span class=&#34;math inline&#34;&gt;\(\Delta\)&lt;/span&gt; is less than &lt;span class=&#34;math inline&#34;&gt;\(0.01\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In any case, since we do not know the “true” alternative &lt;span class=&#34;math inline&#34;&gt;\(p_1\)&lt;/span&gt;, we cannot but set the minimum effect size conservatively. Thus it is possible that the true alternative &lt;span class=&#34;math inline&#34;&gt;\(p_1\)&lt;/span&gt; turns out to be much larger than the boundary &lt;span class=&#34;math inline&#34;&gt;\(p_0 + \Delta\)&lt;/span&gt;. In this case, the minimum sample size we computed can be very larger than what we could have used to detect the alternative at the same level of detection power.&lt;/p&gt;
&lt;p&gt;To detour this issue, it is common practice to conduct a preliminary study with a small sample size to get a rough estimate of &lt;span class=&#34;math inline&#34;&gt;\(p_1\)&lt;/span&gt; and use it to compute the sample size of the main follow-up study. However, designing a sample efficient preliminary study is itself a non-trivial problem since we cannot reuse samples in the preliminary study for testing the main follow-up experiment.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;b-sequential-tests-can-choose-the-sample-size-adaptively.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;b) Sequential tests can choose the sample size adaptively.&lt;/h3&gt;
&lt;p&gt;If there are only two distributions specified in null and alternative hypotheses then similar to the Neyman-Pearson lemma, &lt;a href=&#34;https://en.wikipedia.org/wiki/Sequential_probability_ratio_test&#34;&gt;Wald’s sequential probability ratio test (SPRT)&lt;/a&gt; is the test having the smallest average sample size among all statistical tests including both fixed and sequential tests of pre-specified test level &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; and minimum power level &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;. It is not contradicting the Neyman-Pearson lemma which only covers all fixed sample size tests. In the sequential test, the sample size is a random stopping time at which we declare whether to reject the null or not.&lt;/p&gt;
&lt;p&gt;For our Bernoulli example with &lt;span class=&#34;math inline&#34;&gt;\(H_0: p = p_0\)&lt;/span&gt; vs &lt;span class=&#34;math inline&#34;&gt;\(H_1: p = p_1\)&lt;/span&gt;, the Wald’s SPRT is given by the following procedure.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Set &lt;span class=&#34;math inline&#34;&gt;\(S_0 := 0\)&lt;/span&gt; and for each observation &lt;span class=&#34;math inline&#34;&gt;\(X_n\)&lt;/span&gt;, compute the probability ratio &lt;span class=&#34;math inline&#34;&gt;\(\Lambda_n\)&lt;/span&gt; by
&lt;span class=&#34;math display&#34;&gt;\[
\Lambda_n := \left(\frac{p_1}{p_0}\right)^{X_n}\left(\frac{1-p_1}{1-p_0}\right)^{1-X_n}.
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Update &lt;span class=&#34;math inline&#34;&gt;\(S_n := S_{n-1} + \log \Lambda_n\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Make one of three following decisions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(S_n \geq b\)&lt;/span&gt; then stop and reject the null (since there is only one alternative, it is equivalent to accept the alternative).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(S_n \leq a\)&lt;/span&gt; then stop and reject the alternative (or accept the null).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Otherwise, continue to the next iteration.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here, thresholds &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; are given approximately &lt;span class=&#34;math inline&#34;&gt;\(a \sim \log\frac{1-\beta}{1-\alpha}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(b \sim \log\frac{\beta}{\alpha}\)&lt;/span&gt; for pre-specified test level &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; and minimum power level &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;. Exact thresholds depend on the underlying null and alternative distributions. In the following R example, we use conservative but correct thresholds given by &lt;span class=&#34;math inline&#34;&gt;\(a = \log(1-\beta)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(b = \log\frac{1}{\alpha}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Example R script for SPRT under the null&lt;/em&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1)
max_iter &amp;lt;- 1e+3L
p0 &amp;lt;- 0.5 # Null distribution
p1 &amp;lt;- 0.6 # Alternative distribution
alpha &amp;lt;- 0.05 # Test level
beta &amp;lt;- 0.95 # Minimum power level  
a &amp;lt;- log(1 - beta)
b &amp;lt;- log(1 / alpha)
s &amp;lt;- 0
# Set a placeholder only for visualization.
# Actual test does not require it. 

p &amp;lt;- p0 # True distribution is the alternative
s_history &amp;lt;- rep(NA, max_iter) 
for (n in 1:max_iter) {
  # Observe a new sample
  x &amp;lt;- rbinom(1, 1, p)
  # Update S_n
  s &amp;lt;- s + ifelse(x == 1, log(p1/p0), log((1-p1)/(1-p0)))
  s_history[n] &amp;lt;- s
  # Make a decision
  if (s &amp;gt;= b) {
    decision &amp;lt;- &amp;quot;Reject the null&amp;quot;
    break 
  } else if (s &amp;lt;= a) {
    decision &amp;lt;- &amp;quot;Reject the alternative&amp;quot;
    break
  } 
  if (n == max_iter) {
    decision &amp;lt;- &amp;quot;Test reached the max interation&amp;quot;
  }
}
printf(&amp;quot;SPRT was ended at n=%i&amp;quot;, n)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;SPRT was ended at n=90&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;printf(&amp;quot;Decision: %s&amp;quot;, decision)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Decision: Reject the alternative&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;s_history &amp;lt;- s_history[!is.na(s_history)]
plot(seq_along(s_history), s_history, type = &amp;quot;l&amp;quot;,
     xlab = &amp;quot;n&amp;quot;, ylab = expression(&amp;#39;S&amp;#39;[&amp;#39;n&amp;#39;]),
     ylim = c(a, b))
abline(h = a, col = 2)
abline(h = b, col = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://shinjaehyeok.github.io/post/statistics/sequential_test_efficiency/1.statistical_test_files/figure-html/binomial_sprt_null-1.png&#34; width=&#34;672&#34; /&gt;
&lt;strong&gt;Fig 1. A sample &lt;span class=&#34;math inline&#34;&gt;\(S_n\)&lt;/span&gt; path of SPRT under the null.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Example R script for SPRT under the alternative&lt;/em&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;s &amp;lt;- 0
# Set a placeholder only for visualization.
# Actual test does not require it. 
p &amp;lt;- p1 # True distribution is the alternative
s_history &amp;lt;- rep(NA, max_iter) 
for (n in 1:max_iter) {
  # Observe a new sample
  x &amp;lt;- rbinom(1, 1, p)
  # Update S_n
  s &amp;lt;- s + ifelse(x == 1, log(p1/p0), log((1-p1)/(1-p0)))
  s_history[n] &amp;lt;- s
  # Make a decision
  if (s &amp;gt;= b) {
    decision &amp;lt;- &amp;quot;Reject the null&amp;quot;
    break 
  } else if (s &amp;lt;= a) {
    decision &amp;lt;- &amp;quot;Reject the alternative&amp;quot;
    break
  } 
  if (n == max_iter) {
    decision &amp;lt;- &amp;quot;Test reached the max interation&amp;quot;
  }
}
printf(&amp;quot;SPRT was ended at n=%i&amp;quot;, n)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;SPRT was ended at n=119&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;printf(&amp;quot;Decision: %s&amp;quot;, decision)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Decision: Reject the null&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;s_history &amp;lt;- s_history[!is.na(s_history)]
plot(seq_along(s_history), s_history, type = &amp;quot;l&amp;quot;,
     xlab = &amp;quot;n&amp;quot;, ylab = expression(&amp;#39;S&amp;#39;[&amp;#39;n&amp;#39;]),
     ylim = c(a, b))
abline(h = a, col = 2)
abline(h = b, col = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://shinjaehyeok.github.io/post/statistics/sequential_test_efficiency/1.statistical_test_files/figure-html/binomial_sprt_alter-1.png&#34; width=&#34;672&#34; /&gt;
&lt;strong&gt;Fig 2. A sample &lt;span class=&#34;math inline&#34;&gt;\(S_n\)&lt;/span&gt; path of SPRT under the alternative.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;From the above example, We can see SPRT ended around &lt;span class=&#34;math inline&#34;&gt;\(90\sim120\)&lt;/span&gt;. However, to achieve the same test level and minimum power, LRT (binomial test in this case), requires at least &lt;span class=&#34;math inline&#34;&gt;\(n = 280\)&lt;/span&gt; samples.&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Sample size calculation for the Binomial test&lt;/em&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n_to_power &amp;lt;- function(n, p0, p1, alpha) {
  n &amp;lt;- ceiling(n)
  thres &amp;lt;- qbinom(alpha, n, p0, lower.tail = FALSE)
  pwr &amp;lt;- pbinom(thres, n, p1, lower.tail = FALSE)
  return(pwr)
}

power_to_n &amp;lt;- function(beta, p0, p1, alpha) {
  f &amp;lt;- function(n) {
    beta - n_to_power(n, p0, p1, alpha)
  }
  root &amp;lt;- stats::uniroot(f, c(1, 1e+4), tol = 1e-6)
  return(ceiling(root$root))
}

alpha &amp;lt;- 0.05
beta &amp;lt;- 0.95
p0 &amp;lt;- 0.5
p1 &amp;lt;- 0.6
minimum_sample &amp;lt;- power_to_n(beta, p0 , p1 , alpha)
printf(&amp;quot;%i is the minimum sample size to achieve test level %.2f and power %.2f.&amp;quot;, 
       minimum_sample, alpha, beta)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;280 is the minimum sample size to achieve test level 0.05 and power 0.95.&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Unlike the LRT, the efficiency of Wald’s SPRT is guaranteed only for a simple alternative hypothesis space in which there is a single alternative distribution. However, SPRT works reasonably well even for misspecified alternative distributions.&lt;a href=&#34;#fn4&#34; class=&#34;footnote-ref&#34; id=&#34;fnref4&#34;&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;c-simulations&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;c) Simulations&lt;/h3&gt;
&lt;p&gt;We end this post by checking the sample efficiency of SPRT over LRT in Bernoulli case simulations. Thoughout the simulation, we use test level &lt;span class=&#34;math inline&#34;&gt;\(\alpha = 0.05\)&lt;/span&gt;, minimum power &lt;span class=&#34;math inline&#34;&gt;\(\beta = 0.95\)&lt;/span&gt; for hypotheses &lt;span class=&#34;math inline&#34;&gt;\(H_0: p = 0.5\)&lt;/span&gt; vs &lt;span class=&#34;math inline&#34;&gt;\(H_1: p = 0.6\)&lt;/span&gt;. In this case, the mimum sample size of LRT (one-sided binomial test) is equal to &lt;span class=&#34;math inline&#34;&gt;\(n = 280\)&lt;/span&gt;. Later, we also simulate the misspecified alternatives &lt;span class=&#34;math inline&#34;&gt;\(p = 0.55\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(p = 0.65\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Set up&lt;/em&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Set up
alpha &amp;lt;- 0.05
p0 &amp;lt;- 0.5
p1 &amp;lt;- 0.6
beta &amp;lt;- 0.95
n &amp;lt;- power_to_n(beta, p0, p1, alpha) # n = 254
max_iter &amp;lt;- 1e+4L

# One-sided Binomial test 
run_binom_test &amp;lt;- function(n, p_true,
                           p0 = 0.5, p1 = 0.6,
                           alpha = 0.05) {
  thres &amp;lt;- qbinom(alpha, n, p0, lower.tail = FALSE)
  num_head &amp;lt;- rbinom(1, size = n, prob = p_true)
  return(num_head &amp;gt; thres)
}

# SPRT
run_wald_sprt &amp;lt;- function(p_true, p0 = 0.5, p1 = 0.6,
                      alpha = 0.05, beta = 0.95, max_iter = 1e+3L) {
  a &amp;lt;- log(1 - beta)
  b &amp;lt;- log(1 / alpha)
  s &amp;lt;- 0
  for (n in 1:max_iter) {
    # Observe a new sample
    x &amp;lt;- rbinom(1, 1, p_true)
    # Update S_n
    s &amp;lt;- s + ifelse(x == 1, log(p1/p0), log((1-p1)/(1-p0)))
    # Make a decision
    if (s &amp;gt;= b) {
      decision &amp;lt;- &amp;quot;Reject the null&amp;quot;
      is_reject_null &amp;lt;- TRUE
      break 
    } else if (s &amp;lt;= a) {
      decision &amp;lt;- &amp;quot;Reject the alternative&amp;quot;
      is_reject_null &amp;lt;- FALSE
      break
    } 
    if (n == max_iter) {
      decision &amp;lt;- &amp;quot;Test reached the max interation&amp;quot;
      is_reject_null &amp;lt;- FALSE
    }
  } 
  return(list(
    stopped_n = n,
    decision = decision,
    is_reject_null = is_reject_null
  ))
}&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;i.-under-the-null-h_0-p-0.5&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;i. Under the null (&lt;span class=&#34;math inline&#34;&gt;\(H_0: p = 0.5\)&lt;/span&gt;)&lt;/h4&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1)
# Under the null p_true = 0.5
# LRT (binomial)
binom_null_err &amp;lt;- mean(replicate(max_iter, {
  run_binom_test(n, p_true = p0,
                 p0, p1,
                 alpha)
}))
printf(&amp;quot;Type 1 error of LRT (n = %i) is %.2f&amp;quot;, n, binom_null_err)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Type 1 error of LRT (n = 280) is 0.05&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# SPRT 
sprt_null_out &amp;lt;- replicate(max_iter, {
  out &amp;lt;- run_wald_sprt(p_true = p0, p0, p1,
                alpha, beta, max_iter = 1e+3L)
  return(c(stopped_n = out$stopped_n, is_reject_null = out$is_reject_null))
})
sprt_null_err &amp;lt;- mean(sprt_null_out[&amp;quot;is_reject_null&amp;quot;, ])
sprt_null_sample_size &amp;lt;- mean(sprt_null_out[&amp;quot;stopped_n&amp;quot;, ])
printf(&amp;quot;Type 1 error of SPRT (E[N] = %.1f) is %.2f&amp;quot;, sprt_null_sample_size, sprt_null_err)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Type 1 error of SPRT (E[N] = 137.7) is 0.04&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Under the null, both LRT and SPRT control type-1 error, and SPRT has a smaller average sample size compared to the LRT.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ii.-under-the-alternative-h_1-p-0.6&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;ii. Under the alternative (&lt;span class=&#34;math inline&#34;&gt;\(H_1: p = 0.6\)&lt;/span&gt;)&lt;/h4&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1)
# Under the alternative p_true = 0.6
# LRT (binomial)
binom_power &amp;lt;- mean(replicate(max_iter, {
  run_binom_test(n, p_true = p1,
                 p0, p1,
                 alpha)
}))
printf(&amp;quot;Power of LRT (n = %i) is %.2f&amp;quot;, n, binom_power)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Power of LRT (n = 280) is 0.96&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# SPRT
sprt_power_out &amp;lt;- replicate(max_iter, {
  out &amp;lt;- run_wald_sprt(p_true = p1, p0, p1,
                       alpha, beta, max_iter = 1e+3L)
  return(c(stopped_n = out$stopped_n, is_reject_null = out$is_reject_null))
})
sprt_power &amp;lt;- mean(sprt_power_out[&amp;quot;is_reject_null&amp;quot;, ])
sprt_power_sample_size &amp;lt;- mean(sprt_power_out[&amp;quot;stopped_n&amp;quot;, ])
printf(&amp;quot;Power of SPRT (E[N] = %.1f) is %.2f&amp;quot;, sprt_power_sample_size, sprt_power)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Power of SPRT (E[N] = 139.3) is 0.96&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Under the alternative, both LRT and SPRT achieve the minimum power and SPRT has a smaller average sample size compared to the LRT.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;iii.-under-a-misspecified-alternative-1.-small-p-0.55.&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;iii. Under a misspecified alternative 1. small &lt;span class=&#34;math inline&#34;&gt;\(p = 0.55\)&lt;/span&gt;.&lt;/h4&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1)
# Under a misspecified alternative 1. small p_true = 0.55.
p_mis1 &amp;lt;- 0.55

# LRT (binomial)
binom_power_mis1 &amp;lt;- mean(replicate(max_iter, {
  run_binom_test(n, p_true = p_mis1,
                 p0, p1,
                 alpha)
}))
printf(&amp;quot;Power of LRT (n = %i) is %.2f&amp;quot;, n, binom_power_mis1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Power of LRT (n = 280) is 0.52&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# SPRT
sprt_power_mis1_out &amp;lt;- replicate(max_iter, {
  out &amp;lt;- run_wald_sprt(p_true = p_mis1, p0, p1,
                       alpha, beta, max_iter = 1e+3L)
  return(c(stopped_n = out$stopped_n, is_reject_null = out$is_reject_null))
})
sprt_power_mis1 &amp;lt;- mean(sprt_power_mis1_out[&amp;quot;is_reject_null&amp;quot;, ])
sprt_power_mis_1_sample_size &amp;lt;- mean(sprt_power_mis1_out[&amp;quot;stopped_n&amp;quot;, ])
printf(&amp;quot;Power of SPRT (E[N] = %.1f) is %.2f&amp;quot;, sprt_power_mis_1_sample_size, sprt_power_mis1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Power of SPRT (E[N] = 234.5) is 0.50&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Under a misspecified alternative with &lt;span class=&#34;math inline&#34;&gt;\(p = 0.55 &amp;lt; p_1 = 0.6\)&lt;/span&gt;, both LRT and SPRT achieve a similar power but SPRT has a smaller average sample size. The achieved power is below the pre-specified minimum power as the true success probability is smaller than the alternative.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;iv.-under-a-misspecified-alternative-2.-large-p-0.65.&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;iv. Under a misspecified alternative 2. large &lt;span class=&#34;math inline&#34;&gt;\(p = 0.65\)&lt;/span&gt;.&lt;/h4&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1)
# Under a misspecified alternative 2. large p_true = 0.65.
p_mis2 &amp;lt;- 0.65

# LRT (binomial)
binom_power_mis2 &amp;lt;- mean(replicate(max_iter, {
  run_binom_test(n, p_true = p_mis2,
                 p0, p1,
                 alpha)
}))
printf(&amp;quot;Power of LRT (n = %i) is %.2f&amp;quot;, n, binom_power_mis2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Power of LRT (n = 280) is 1.00&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sprt_power_mis2_out &amp;lt;- replicate(max_iter, {
  out &amp;lt;- run_wald_sprt(p_true = p_mis2, p0, p1,
                       alpha, beta, max_iter = 1e+3L)
  return(c(stopped_n = out$stopped_n, is_reject_null = out$is_reject_null))
})
sprt_power_mis2 &amp;lt;- mean(sprt_power_mis2_out[&amp;quot;is_reject_null&amp;quot;, ])
sprt_power_mis_2_sample_size &amp;lt;- mean(sprt_power_mis2_out[&amp;quot;stopped_n&amp;quot;, ])
printf(&amp;quot;Power of SPRT (E[N] = %.1f) is %.2f&amp;quot;, sprt_power_mis_2_sample_size, sprt_power_mis2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Power of SPRT (E[N] = 76.1) is 1.00&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Under a misspecified alternative with &lt;span class=&#34;math inline&#34;&gt;\(p = 0.65 &amp;gt; p_1 = 0.6\)&lt;/span&gt;, both LRT and SPRT have almost a power of 1 but SPRT has a much smaller average sample size as SPRT can early stop the procedure adaptively to the underlying higher success probability than the alternative.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;Sequential hypothesis testing procedures can achieve a better sample efficiency even compared to the best-fixed sample size test. In the following post, we will explain the flexibility and safety of sequential testing procedures.&lt;/p&gt;
&lt;!--
## 1. Fixed sample size

In the classical setting like scientific researches or survey analyses, it is often expensive to collect observations. Therefore, the sample size is often determined by external factors such as research budget or the number of available respondents at the survey time rather than the power analysis result. Even sometimes, a pre-collected set of samples is given to analysts and analysts themselves have no or very small control on the collected sample size. This is a reason why, in many textbook scenarios like above, samples of a fixed number are given and we are asked to perform fixed sample size tests. 


## 2. Sequential observations
In industrial settings, data are often collected by sensors or logs from which we can access a stream of observations in real time. 

 This is a classical example of statistical hypothesis testing and there are various standard ways to perform a test including Z-test (asymptotic test for a large sample size) and binomial test (exact test for small-to-medium sample  sizes). --&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;Since &lt;span class=&#34;math inline&#34;&gt;\(\bar{X}_n\)&lt;/span&gt; is discrete, there might be no such constant &lt;span class=&#34;math inline&#34;&gt;\(c_\alpha\)&lt;/span&gt; satisfying the equality. In this case, we can randomized the test, which is beyond the scope of this post.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;Computing the critical value or the corresponding p-value could be computationally expensive if the sample size is large. In this case, we can use a normal approximation-based z-test instead of the exact Binomial test.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;This is not a free lunch - in worst scenarios, SPRT can reach the max iteration which could be much larger than the fixed sample size of LRT.&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn4&#34;&gt;&lt;p&gt;We can use a mixture of SPRTs to achieve a near optimal sample efficiency for a composite alternative hypothesis space in which a range of alternative distributions exist. This topic has been discussed in literature. For example, see &lt;a href=&#34;https://arxiv.org/abs/2010.08082&#34;&gt;arXiv&lt;/a&gt;.&lt;a href=&#34;#fnref4&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
