[{"authors":null,"categories":null,"content":"I am a Data Scientist at Google. Previously, I was a postdoctoral researcher in the Department of Statistics at Carnegie Mellon University, working with Aaditya Ramdas and Alessandro Rinaldo. I recently graduated from the same department with a Ph.D. in Statistics.\nResearch My research interest lies in understanding the sequential and adaptive nature of data analysis. I study how commonly used statistical inference procedures behave under the presence of an analyst\u0026rsquo;s data-dependent choices. My current projects focus on designing and analyzing nonasymptotic sequential testings and online change-point detection procedures.\nKeywords: Always-valid inference, Sequential test, Multi-armed bandit, Change-point detection\nRecent Papers E-detectors: a nonparametric framework for online changepoint detection\nJ. Shin, A. Ramdas, A. Rinaldo\nPreprint arXiv\nNonparametric iterated-logarithm extensions of the sequential generalized likelihood ratio test\nJ. Shin, A. Ramdas, A. Rinaldo\nIEEE Journal on Selected Areas in Information Theory arXiv, code\nOn conditional versus marginal bias in multi-armed bandits\nJ. Shin, A. Ramdas, A. Rinaldo\nThirty-seventh International Conference on Machine Learning (ICML 2020) arXiv\nAre sample means in multi-armed bandits positively or negatively biased?\nJ. Shin, A. Ramdas, A. Rinaldo\nNeural Information Processing Systems (NeurIPS 2019, Spotlight) arXiv\nOn the bias, risk and consistency of sample means in multi-armed bandits\nJ. Shin, A. Ramdas, A. Rinaldo\nSIAM Journal on Mathematics of Data Science arXiv proc\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://shinjaehyeok.github.io/author/jaehyeok-shin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/jaehyeok-shin/","section":"authors","summary":"I am a Data Scientist at Google. Previously, I was a postdoctoral researcher in the Department of Statistics at Carnegie Mellon University, working with Aaditya Ramdas and Alessandro Rinaldo. I recently graduated from the same department with a Ph.","tags":null,"title":"Jaehyeok Shin","type":"authors"},{"authors":[],"categories":["Statistics"],"content":"\r\rIn this follow-up post, we explain another advantage of sequential hypothesis testing: Flexibility and safety. Please visit the previous post for an introduction to Wald’s sequential probability ratio test (SPRT) and a discussion of its sample efficiency.\nCan we early stop the testing if the p-value already reached below \\(\\alpha\\)?\rAgain, let’s start with a simple coin toss example where we observe a sequence of independent observations \\(X_1, X_2, \\dots \\in \\{0,1\\}\\) from a Bernoulli distribution \\(B(p)\\) with \\(p \\in (0,1)\\). We are interested in testing whether the underlying coin is pair (\\(H_0: p = 0.5\\)) or biased toward to head (\\(H_1: p \u0026gt; 0.5\\)).\nFrom the previous post, we know that the binomial test is the best fixed sample size test. Formally, let’s set up the binomial test of level \\(\\alpha = 0.05\\) with the minimum power of \\(\\beta = 0.95\\) at \\(p = 0.6\\). In this case, the minimum sample size is given by the following R script.\nset.seed(1)\rprintf \u0026lt;- function(...) invisible(print(sprintf(...)))\rn_to_power \u0026lt;- function(n, p0, p1, alpha) {\rn \u0026lt;- ceiling(n)\rthres \u0026lt;- qbinom(alpha, n, p0, lower.tail = FALSE) # Reject null if s \u0026gt; thres\rpwr \u0026lt;- pbinom(thres, n, p1, lower.tail = FALSE)\rreturn(pwr)\r}\rpower_to_n \u0026lt;- function(beta, p0, p1, alpha) {\rf \u0026lt;- function(n) {\rbeta - n_to_power(n, p0, p1, alpha)\r}\rroot \u0026lt;- stats::uniroot(f, c(1, 1e+4), tol = 1e-6)\rreturn(ceiling(root$root))\r}\ralpha \u0026lt;- 0.05\rbeta \u0026lt;- 0.95\rp0 \u0026lt;- 0.5\rp1 \u0026lt;- 0.6\rn \u0026lt;- power_to_n(beta, p0 , p1 , alpha)\rprintf(\u0026quot;%i is the minimum sample size to achieve test level %.2f and power %.2f at p= %.1f against the null p=%.2f.\u0026quot;, n, alpha, beta, p1, p0)\r## [1] \u0026quot;280 is the minimum sample size to achieve test level 0.05 and power 0.95 at p= 0.6 against the null p=0.50.\u0026quot;\rIn the standard fixed sample size setting, we compute the p-value once collecting all 280 samples. Then, if the p-value is less than the test level \\(\\alpha\\), we reject the null hypothesis. In this case, we can control the type-1 error below \\(\\alpha\\) and achieve the minimum power \\(\\beta\\) under the alternative (\\(p = 0.6\\)), as the following simulation demonstrates.\nbinom_p_value \u0026lt;- function(s, n, p0) {\rreturn(pbinom(s-1, n, p0, lower.tail = FALSE))\r}\rrun_binom_simul \u0026lt;- function(p_true, n, p0, alpha, max_iter) {\r# We shorten the simulation by using the fact X_1 + ...+X_n ~ Binom(n, p)\rs_vec \u0026lt;- rbinom(max_iter, size = n, prob = p_true)\rp_vec \u0026lt;- sapply(s_vec, binom_p_value, n = n, p0 = p0)\rreturn(mean(p_vec \u0026lt;= alpha))\r}\r# Under the null\rmax_iter \u0026lt;- 1e+4L\rtype_1_err \u0026lt;- run_binom_simul(p_true = p0, n, p0, alpha, max_iter)\rprintf(\u0026quot;Type-1 error of the binomial test is %.2f.\u0026quot;, type_1_err)\r## [1] \u0026quot;Type-1 error of the binomial test is 0.04.\u0026quot;\r# Under the alternative\rpwr \u0026lt;- run_binom_simul(p_true = p1, n, p0, alpha, max_iter)\rprintf(\u0026quot;Power of the binomial test at p=%.1f is %.2f.\u0026quot;, p1, pwr)\r## [1] \u0026quot;Power of the binomial test at p=0.6 is 0.95.\u0026quot;\rWhat if we observe the coin toss one by one and we compute the p-value at each time whenever a new coin toss happens? In the previous post, we noticed that Wald’s sequential probability ratio test (SPRT) can achieve a high sample efficiency by adaptively stopping the test procedure. Why not do the same trick to the binomial test? Can we early stop the testing if the p-value already reached below \\(\\alpha\\)? Let’s run a simulation to check whether this early stopping still results in a type-1 error below the test level \\(\\alpha\\).\nrun_binom_early_stop \u0026lt;- function(p_true, n, p0, alpha) {\rs \u0026lt;- 0\rfor (i in 1:n) {\rs \u0026lt;- s + rbinom(1, size = 1, prob = p_true)\rp \u0026lt;- binom_p_value(s, i, p0)\rif (p \u0026lt;= alpha) {\rreturn(TRUE)\r}\r}\rreturn(FALSE)\r}\r# Under the null\rearly_stopp_type_1_err \u0026lt;- mean(replicate(max_iter,{\rrun_binom_early_stop(p_true = 0.5, n, p0, alpha)\r}))\rprintf(\u0026quot;Type-1 error of the early stopped binomial test is %.2f.\u0026quot;, early_stopp_type_1_err)\r## [1] \u0026quot;Type-1 error of the early stopped binomial test is 0.26.\u0026quot;\rIt turns out that the early stopping strategy increased the type 1 error significantly above the target level. This inflation of type-1 error is an example of p-hacking. This issue is getting worse if we run a longer test.\nset.seed(4)\rs_vec \u0026lt;- cumsum(rbinom(n, size = 1, prob = 0.5))\rf \u0026lt;- function(i) binom_p_value(s_vec[i], i, p0)\rp_vec \u0026lt;- sapply(seq_along(s_vec), f)\rplot(seq_along(p_vec), p_vec, type = \u0026quot;l\u0026quot;, xlab= \u0026quot;n\u0026quot;, ylab =\u0026quot;p-value\u0026quot;)\rabline(h = alpha, col = 2)\rFig. 1 An example sample path of p-values hitting the threshold \\(\\alpha\\) although the underlying samples generated under the null. The red horizontal line corresponds to the test level \\(\\alpha\\). \n\rCan we continue to collect more samples if the p-value at the end is slightly above \\(\\alpha\\)?\rOkay, we learned that we should not early stop and wait to collect all samples of the pre-calculated size. But what if the p-value is slightly above \\(\\alpha = 0.05\\) - let’s say \\(p = 0.15\\). Maybe, if we would have collected a bit more samples then the p-value might possibly be less than \\(\\alpha\\) and we could reject the null. Can we continue to collect more samples to see whether the p-value becomes less than \\(\\alpha\\) or goes away from it?\nLet’s run a simple simulation to answer the question. For each run, we first collect the minimum sample size \\(n\\) computed to achieve the minimum power \\(\\beta = 0.95\\). Then, compute the p-value based on the collected \\(n\\) samples. If the p-value is less than \\(0.2\\) then we collect another \\(n\\) samples and re-compute the p-value based on \\(2n\\) samples. Finally, we reject the null if the final p-value is less than or equal to the test level \\(\\alpha\\).\nrun_binom_a_bit_more_simul \u0026lt;- function(p_true, n, p0, alpha, max_iter) {\r# We shorten the simulation by using the fact X_1 + ...+X_n ~ Binom(n, p)\rs_vec \u0026lt;- rbinom(max_iter, size = n, prob = p_true)\rp_vec \u0026lt;- sapply(s_vec, binom_p_value, n = n, p0 = p0)\r# If p-value \u0026gt; alpha and \u0026lt; 0.2, collect n more samples\rabove_alpha_ind \u0026lt;- which(p_vec \u0026gt; alpha \u0026amp; p_vec \u0026lt; 0.2)\rs_vec[above_alpha_ind] \u0026lt;- s_vec[above_alpha_ind] + rbinom(length(above_alpha_ind), size = n, prob = p_true)\rp_vec[above_alpha_ind] \u0026lt;- sapply(s_vec[above_alpha_ind], binom_p_value, n = 2*n, p0 = p0)\rreturn(mean(p_vec \u0026lt;= alpha))\r}\ra_bit_more_type_1_err \u0026lt;- run_binom_a_bit_more_simul(p_true = p0, n, p0, alpha, max_iter)\rprintf(\u0026quot;Type-1 error of the binomial test with the contional continuation is %.3f.\u0026quot;, a_bit_more_type_1_err)\r## [1] \u0026quot;Type-1 error of the binomial test with the contional continuation is 0.063.\u0026quot;\rIn this case, the type-1 error increased above the test level \\(\\alpha\\) though it is not as dramatic as the early stopping case. If we continuously collect and compute p-values rather than collect a single batch then the inflation of the type-1 error becomes more significant. This type of continuation of the testing procedure is also an example of p-hacking.\nrun_binom_a_bit_more2_run \u0026lt;- function(p_true, n, p0, alpha) {\rs \u0026lt;- rbinom(1, size = n, prob = p_true)\rp \u0026lt;- binom_p_value(s, n, p0)\rif (p \u0026lt;= alpha) {\r# If first n samples give p-value less than alpha\r# Reject the null\rreturn(TRUE)\r} else if (p \u0026gt;= 0.2) {\r# If first n samples give p-value above 0.2\r# Fail to reject the null and stop the test\rreturn(FALSE)\r}\r# Otherwise, continue the test until we collect another n samples.\rfor (i in 1:n) {\rs \u0026lt;- s + rbinom(1, size = 1, prob = p_true)\rp \u0026lt;- binom_p_value(s, i, p0)\rif (p \u0026lt;= alpha) {\rreturn(TRUE)\r}\r}\rreturn(FALSE)\r}\r# Under the null\ra_bit_more2_type_1_err \u0026lt;- mean(replicate(max_iter,{\rrun_binom_a_bit_more2_run(p_true = 0.5, n, p0, alpha)\r}))\rprintf(\u0026quot;Type-1 error of the binomial test with the additional continous monitoring is %.2f.\u0026quot;, a_bit_more2_type_1_err)\r## [1] \u0026quot;Type-1 error of the binomial test with the additional continous monitoring is 0.18.\u0026quot;\r\rAlways-valid p-values - flexibly and safely early stop or continue the test.\rSo far, we have observed how the early stop or continual test results in the type-1 error inflation for fixed sample size tests. The root cause of this inflation is that the p-value from a fixed sample size test is valid only at the pre-specified fixed sample size. In other words, let \\(p_n\\) be the p-value of \\(n\\) samples. Then we have\r\\[\rP_{H_0}\\left(p_n \\leq \\alpha\\right) \\leq \\alpha, ~~\\forall n.\r\\]\rHowever, to early stop or continue the test without the type-1 error inflation, we need the following stronger inequality.\r\\[\rP_{H_0}\\left(\\exists n: p_n \\leq \\alpha\\right) \\leq \\alpha.\r\\]\rNote that fixed sample based p-values do not necessarily satisfy the above strong inequality.1\nIf a random sequence \\(\\{p_n\\}_{n \\geq 1}\\) satisfies the above stronger inequality then it is called “always-valid p-value”. Every sequential hypothesis test has the corresponding always-valid p-value. Therefore, we can flexibly and safely early stop or continue the test.\nAs a concrete example, let’s compute the always-valid p-value of the Wald’s SPRT. Recall that for our Bernoulli example with \\(H_0: p = p_0\\) vs \\(H_1: p = p_1\\), the Wald’s SPRT is a sequential hypothesis testing procedures defined as follows.\r* Set \\(S_0 := 0\\) and for each observation \\(X_n\\), compute the probability ratio \\(\\Lambda_n\\) by\r\\[\r\\Lambda_n := \\left(\\frac{p_1}{p_0}\\right)^{X_n}\\left(\\frac{1-p_1}{1-p_0}\\right)^{1-X_n}.\r\\]\n\rUpdate \\(S_n := S_{n-1} + \\log \\Lambda_n\\)\n\rMake one of three following decisions:\n\rIf \\(S_n \\geq b\\) then stop and reject the null (since there is only one alternative, it is equivalent to accept the alternative).\n\rIf \\(S_n \\leq a\\) then stop and reject the alternative (or accept the null).\n\rOtherwise, continue to the next iteration.\n\r\r\rHere, we can set \\(a = \\log(1-\\beta)\\) and \\(b = \\log\\frac{1}{\\alpha}\\). In this case, we can construct an always-valid p-value by\r\\[\rp_n := \\min\\left\\{1, e^{-S_n}\\right\\}.\r\\]\rWe can easily check \\(p_n \\leq \\alpha \\Leftrightarrow S_n \\geq b = \\log(1/\\alpha)\\). Therefore, in terms of the rejection of the null, tracking \\(S_n\\) with the threshold \\(\\log(1/\\alpha)\\) of the Wald’s SPRT is equivalent to tracking \\(p_n\\) with the threshold \\(\\alpha\\). From the type-1 error control of the Wald’s SPRT, we can check \\(p_n\\) is an always-valid p-value satisfying the above stronger inequality.\nAs a remark, the always-valid p-value above is not uniformly distributed. In fact, we can prove that \\(p_n \\rightarrow 1\\) almost surely under the null and \\(p_n \\rightarrow 0\\) almost surely under the alternative as \\(n \\to \\infty\\). As it is not uniformly distributed, the standard interpretation of the strength of evidence in terms of p-value is not applicable. In fact, though tracking \\(S_n\\) and \\(p_n\\) are equivalent, it is recommended to track \\(S_n\\) directly as \\(\\mathbb{E}S_n = \\mathbb{E}[N] \\mathrm{KL}(p_1||p_0)\\) is the information gain under the alternative.\nset.seed(1)\rsprt_p_value \u0026lt;- function(p_true, p0 = 0.5,\rp1 = 0.6,\rmax_iter = 1e+3L){\rs \u0026lt;- 0\r# Set a placeholder only for visualization.\r# We let the p-value path reaches the maximum to see the convergence\rp_val_history \u0026lt;- rep(NA, max_iter) for (n in 1:max_iter) {\r# Observe a new sample\rx \u0026lt;- rbinom(1, 1, p_true)\r# Update S_n\rs \u0026lt;- s + ifelse(x == 1, log(p1/p0), log((1-p1)/(1-p0)))\rp_val_history[n] \u0026lt;- min(c(1,exp(-s)))\r}\rreturn(p_val_history)\r}\r# Under the null\rnull_p_val \u0026lt;- sprt_p_value(p_true = 0.5)\rplot(seq_along(null_p_val), null_p_val, type = \u0026quot;l\u0026quot;,\rxlab = \u0026quot;n\u0026quot;, ylab = \u0026#39;p-value\u0026#39;,\rylim = c(0, 1),\rmain = \u0026quot;Under the null\u0026quot;)\rabline(h = alpha, col = 2)\r# Under the alternative\ralter_p_val \u0026lt;- sprt_p_value(p_true = 0.6)\rplot(seq_along(alter_p_val), alter_p_val, type = \u0026quot;l\u0026quot;,\rxlab = \u0026quot;n\u0026quot;, ylab = \u0026#39;p-value\u0026#39;,\rylim = c(0, 1),\rmain = \u0026quot;Under the alternative\u0026quot;)\rabline(h = alpha, col = 2)\rConclusion\rFixed sample size tests can suffer from type-1 error inflation if the test is not stopped at the pre-specified time. In contrast, sequential hypothesis tests have great flexibility that allows researchers to early stop or continue the experiment without inflating type-1 error.\n\r\r\rIn fact, in most standard fixed sample tests, we have \\(P_{H_0}\\left(\\exists n: p_n \\leq \\alpha\\right) = 1\\).↩︎\n\r\r\r","date":1654387200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1654387200,"objectID":"4138676dbcbfdf6cc69599a590297004","permalink":"https://shinjaehyeok.github.io/post/statistics/sequential_test_safety/sequential_test_flexibility_safety/","publishdate":"2022-06-05T00:00:00Z","relpermalink":"/post/statistics/sequential_test_safety/sequential_test_flexibility_safety/","section":"post","summary":"In this follow-up post, we explain another advantage of sequential hypothesis testing: Flexibility and safety. Please visit the previous post for an introduction to Wald’s","tags":["ab test","inference","sequential test"],"title":"Advantages of Sequential Hypothesis Testing: 2. Flexibility and Safety","type":"post"},{"authors":[],"categories":["Statistics"],"content":"\r\rIn this and a follow-up posts, we explain two main advantages of sequential hypothesis testing methods compared to standard tests based on fixed sample size.\nSample efficiency in practice\rAs a working example, suppose we have observed a sequence of independent coin tosses, which are encoded by \\(X_n = 1\\) if the \\(n\\)-th observation is head and \\(X_n = 0\\) if it is tail. Statistically, we can model this sequence by independently and identically distributed (i.i.d.) random observations \\(X_1, X_2, \\dots \\in \\{0,1\\}\\) from a Bernoulli distribution \\(B(p)\\) with unknown success probability \\(p \\in (0,1)\\). How can we test whether the coin is fair, i.e., \\(p = p_0:= 0.5\\) or not. For a simple presentation, we assume that if the coin is biased then the biased success probability \\(p_1\\) must be larger than \\(p_0\\).\na) Difficulties in sample size calculation in fixed sample size tests\rIf we know the success probability \\(p\\) must be equal to a constant \\(p_1 \u0026gt; 0.5\\) when the coin turns out to be unfair or biased then the Neyman-Pearson lemma shows that for a fixed test level \\(\\alpha \\in (0,1)\\), the most powerful test is the likelihood-ratio test (LRT) rejecting the null (\\(H_0: p = p_0\\)) in a favor of the alternative (\\(H_1: p = p_1\\)) if \\(\\bar{X}_n \\geq c_\\alpha\\) where \\(\\bar{X}_n := \\sum_{i=1}^n X_i /n\\) and \\(c_\\alpha\\) is a positive constant satisfying \\(P_{H_0}(\\bar{X}_n \\geq c_\\alpha) = \\alpha\\).1\nIn fact, LRT is a uniformly most powerful test against all possible alternatives \\(p_1\\) larger than \\(p_0\\). Hence, if the sample size and test level are fixed, checking whether the sample mean is larger than the critical value \\(c_\\alpha\\) is all you need to do. This test is also called the exact Binomial test.2\nExample R script\nset.seed(1)\rprintf \u0026lt;- function(...) invisible(print(sprintf(...)))\rn \u0026lt;- 100L\r# Testing fair coin sample (alpha = 0.05)\rfair_coin_sample \u0026lt;- rbinom(n, size = 1, prob = 0.5) fair_coin_test_result \u0026lt;- binom.test(sum(fair_coin_sample), n, p = 0.5, alternative = \u0026quot;greater\u0026quot;)\rprintf(\u0026quot;p-value of a fair coin test is %.3f\u0026quot;, fair_coin_test_result$p.value)\r## [1] \u0026quot;p-value of a fair coin test is 0.691\u0026quot;\r# Testing biased coin sample (alpha = 0.05)\rbiased_coin_sample \u0026lt;- rbinom(n, size = 1, prob = 0.6)\rbiased_coin_test_result \u0026lt;- binom.test(sum(biased_coin_sample), n, p = 0.5, alternative = \u0026quot;greater\u0026quot;)\rprintf(\u0026quot;p-value of a biased coin test is %.3f\u0026quot;, biased_coin_test_result$p.value)\r## [1] \u0026quot;p-value of a biased coin test is 0.028\u0026quot;\rHowever, in practice, we rarely know what the alternative success rate could be before an experiment. Instead, we set a minimum effect size of interest \\(\\Delta \u0026gt; 0\\) that we practically consider as a meaningful bias from \\(p_0\\). Based on the minimum effect size \\(\\Delta\\) and desired minimum power \\(\\beta \\in (0,1)\\), we can compute the minimum sample size we need to perform a statistical test that can detect \\(p_1 \u0026gt;= p_0 + \\Delta\\) with probability at least \\(\\beta\\) if the coin is actually biased.\nThe size of \\(\\Delta\\) depends on the application. For example, If we just play a game at a party with friends, we may still consider a coin with a success probability up to \\(0.51\\) as a “fairly fair” coin we can use. In this case, the minimum effect size is \\(\\Delta = 0.1\\). On the other hand, if we play a serious gamble with a large bet on the coin toss then even \\(0.501\\) can be viewed as an unfair coin and the minimum effect size \\(\\Delta\\) is less than \\(0.01\\).\nIn any case, since we do not know the “true” alternative \\(p_1\\), we cannot but set the minimum effect size conservatively. Thus it is possible that the true alternative \\(p_1\\) turns out to be much larger than the boundary \\(p_0 + \\Delta\\). In this case, the minimum sample size we computed can be very larger than what we could have used to detect the alternative at the same level of detection power.\nTo detour this issue, it is common practice to conduct a preliminary study with a small sample size to get a rough estimate of \\(p_1\\) and use it to compute the sample size of the main follow-up study. However, designing a sample efficient preliminary study is itself a non-trivial problem since we cannot reuse samples in the preliminary study for testing the main follow-up experiment.\n\rb) Sequential tests can choose the sample size adaptively.\rIf there are only two distributions specified in null and alternative hypotheses then similar to the Neyman-Pearson lemma, Wald’s sequential probability ratio test (SPRT) is the test having the smallest average sample size among all statistical tests including both fixed and sequential tests of pre-specified test level \\(\\alpha\\) and minimum power level \\(\\beta\\). It is not contradicting the Neyman-Pearson lemma which only covers all fixed sample size tests. In the sequential test, the sample size is a random stopping time at which we declare whether to reject the null or not.\nFor our Bernoulli example with \\(H_0: p = p_0\\) vs \\(H_1: p = p_1\\), the Wald’s SPRT is given by the following procedure.\n\rSet \\(S_0 := 0\\) and for each observation \\(X_n\\), compute the probability ratio \\(\\Lambda_n\\) by\r\\[\r\\Lambda_n := \\left(\\frac{p_1}{p_0}\\right)^{X_n}\\left(\\frac{1-p_1}{1-p_0}\\right)^{1-X_n}.\r\\]\n\rUpdate \\(S_n := S_{n-1} + \\log \\Lambda_n\\)\n\rMake one of three following decisions:\n\rIf \\(S_n \\geq b\\) then stop and reject the null (since there is only one alternative, it is equivalent to accept the alternative).\n\rIf \\(S_n \\leq a\\) then stop and reject the alternative (or accept the null).\n\rOtherwise, continue to the next iteration.\n\r\r\rHere, thresholds \\(a\\) and \\(b\\) are given approximately \\(a \\sim \\log\\frac{1-\\beta}{1-\\alpha}\\) and \\(b \\sim \\log\\frac{\\beta}{\\alpha}\\) for pre-specified test level \\(\\alpha\\) and minimum power level \\(\\beta\\). Exact thresholds depend on the underlying null and alternative distributions. In the following R example, we use conservative but correct thresholds given by \\(a = \\log(1-\\beta)\\) and \\(b = \\log\\frac{1}{\\alpha}\\).\nExample R script for SPRT under the null\nset.seed(1)\rmax_iter \u0026lt;- 1e+3L\rp0 \u0026lt;- 0.5 # Null distribution\rp1 \u0026lt;- 0.6 # Alternative distribution\ralpha \u0026lt;- 0.05 # Test level\rbeta \u0026lt;- 0.95 # Minimum power level a \u0026lt;- log(1 - beta)\rb \u0026lt;- log(1 / alpha)\rs \u0026lt;- 0\r# Set a placeholder only for visualization.\r# Actual test does not require it. p \u0026lt;- p0 # True distribution is the alternative\rs_history \u0026lt;- rep(NA, max_iter) for (n in 1:max_iter) {\r# Observe a new sample\rx \u0026lt;- rbinom(1, 1, p)\r# Update S_n\rs \u0026lt;- s + ifelse(x == 1, log(p1/p0), log((1-p1)/(1-p0)))\rs_history[n] \u0026lt;- s\r# Make a decision\rif (s \u0026gt;= b) {\rdecision \u0026lt;- \u0026quot;Reject the null\u0026quot;\rbreak } else if (s \u0026lt;= a) {\rdecision \u0026lt;- \u0026quot;Reject the alternative\u0026quot;\rbreak\r} if (n == max_iter) {\rdecision \u0026lt;- \u0026quot;Test reached the max interation\u0026quot;\r}\r}\rprintf(\u0026quot;SPRT was ended at n=%i\u0026quot;, n)\r## [1] \u0026quot;SPRT was ended at n=90\u0026quot;\rprintf(\u0026quot;Decision: %s\u0026quot;, decision)\r## [1] \u0026quot;Decision: Reject the alternative\u0026quot;\rs_history \u0026lt;- s_history[!is.na(s_history)]\rplot(seq_along(s_history), s_history, type = \u0026quot;l\u0026quot;,\rxlab = \u0026quot;n\u0026quot;, ylab = expression(\u0026#39;S\u0026#39;[\u0026#39;n\u0026#39;]),\rylim = c(a, b))\rabline(h = a, col = 2)\rabline(h = b, col = 2)\rFig 1. A sample \\(S_n\\) path of SPRT under the null.\nExample R script for SPRT under the alternative\ns \u0026lt;- 0\r# Set a placeholder only for visualization.\r# Actual test does not require it. p \u0026lt;- p1 # True distribution is the alternative\rs_history \u0026lt;- rep(NA, max_iter) for (n in 1:max_iter) {\r# Observe a new sample\rx \u0026lt;- rbinom(1, 1, p)\r# Update S_n\rs \u0026lt;- s + ifelse(x == 1, log(p1/p0), log((1-p1)/(1-p0)))\rs_history[n] \u0026lt;- s\r# Make a decision\rif (s \u0026gt;= b) {\rdecision \u0026lt;- \u0026quot;Reject the null\u0026quot;\rbreak } else if (s \u0026lt;= a) {\rdecision \u0026lt;- \u0026quot;Reject the alternative\u0026quot;\rbreak\r} if (n == max_iter) {\rdecision \u0026lt;- \u0026quot;Test reached the max interation\u0026quot;\r}\r}\rprintf(\u0026quot;SPRT was ended at n=%i\u0026quot;, n)\r## [1] \u0026quot;SPRT was ended at n=119\u0026quot;\rprintf(\u0026quot;Decision: %s\u0026quot;, decision)\r## [1] \u0026quot;Decision: Reject the null\u0026quot;\rs_history \u0026lt;- s_history[!is.na(s_history)]\rplot(seq_along(s_history), s_history, type = \u0026quot;l\u0026quot;,\rxlab = \u0026quot;n\u0026quot;, ylab = expression(\u0026#39;S\u0026#39;[\u0026#39;n\u0026#39;]),\rylim = c(a, b))\rabline(h = a, col = 2)\rabline(h = b, col = 2)\rFig 2. A sample \\(S_n\\) path of SPRT under the alternative.\nFrom the above example, We can see SPRT ended around \\(90\\sim120\\). However, to achieve the same test level and minimum power, LRT (binomial test in this case), requires at least \\(n = 280\\) samples.3\nSample size calculation for the Binomial test\nn_to_power \u0026lt;- function(n, p0, p1, alpha) {\rn \u0026lt;- ceiling(n)\rthres \u0026lt;- qbinom(alpha, n, p0, lower.tail = FALSE)\rpwr \u0026lt;- pbinom(thres, n, p1, lower.tail = FALSE)\rreturn(pwr)\r}\rpower_to_n \u0026lt;- function(beta, p0, p1, alpha) {\rf \u0026lt;- function(n) {\rbeta - n_to_power(n, p0, p1, alpha)\r}\rroot \u0026lt;- stats::uniroot(f, c(1, 1e+4), tol = 1e-6)\rreturn(ceiling(root$root))\r}\ralpha \u0026lt;- 0.05\rbeta \u0026lt;- 0.95\rp0 \u0026lt;- 0.5\rp1 \u0026lt;- 0.6\rminimum_sample \u0026lt;- power_to_n(beta, p0 , p1 , alpha)\rprintf(\u0026quot;%i is the minimum sample size to achieve test level %.2f and power %.2f.\u0026quot;, minimum_sample, alpha, beta)\r## [1] \u0026quot;280 is the minimum sample size to achieve test level 0.05 and power 0.95.\u0026quot;\rUnlike the LRT, the efficiency of Wald’s SPRT is guaranteed only for a simple alternative hypothesis space in which there is a single alternative distribution. However, SPRT works reasonably well even for misspecified alternative distributions.4\n\rc) Simulations\rWe end this post by checking the sample efficiency of SPRT over LRT in Bernoulli case simulations. Thoughout the simulation, we use test level \\(\\alpha = 0.05\\), minimum power \\(\\beta = 0.95\\) for hypotheses \\(H_0: p = 0.5\\) vs \\(H_1: p = 0.6\\). In this case, the mimum sample size of LRT (one-sided binomial test) is equal to \\(n = 280\\). Later, we also simulate the misspecified alternatives \\(p = 0.55\\) and \\(p = 0.65\\).\nSet up\n# Set up\ralpha \u0026lt;- 0.05\rp0 \u0026lt;- 0.5\rp1 \u0026lt;- 0.6\rbeta \u0026lt;- 0.95\rn \u0026lt;- power_to_n(beta, p0, p1, alpha) # n = 254\rmax_iter \u0026lt;- 1e+4L\r# One-sided Binomial test run_binom_test \u0026lt;- function(n, p_true,\rp0 = 0.5, p1 = 0.6,\ralpha = 0.05) {\rthres \u0026lt;- qbinom(alpha, n, p0, lower.tail = FALSE)\rnum_head \u0026lt;- rbinom(1, size = n, prob = p_true)\rreturn(num_head \u0026gt; thres)\r}\r# SPRT\rrun_wald_sprt \u0026lt;- function(p_true, p0 = 0.5, p1 = 0.6,\ralpha = 0.05, beta = 0.95, max_iter = 1e+3L) {\ra \u0026lt;- log(1 - beta)\rb \u0026lt;- log(1 / alpha)\rs \u0026lt;- 0\rfor (n in 1:max_iter) {\r# Observe a new sample\rx \u0026lt;- rbinom(1, 1, p_true)\r# Update S_n\rs \u0026lt;- s + ifelse(x == 1, log(p1/p0), log((1-p1)/(1-p0)))\r# Make a decision\rif (s \u0026gt;= b) {\rdecision \u0026lt;- \u0026quot;Reject the null\u0026quot;\ris_reject_null \u0026lt;- TRUE\rbreak } else if (s \u0026lt;= a) {\rdecision \u0026lt;- \u0026quot;Reject the alternative\u0026quot;\ris_reject_null \u0026lt;- FALSE\rbreak\r} if (n == max_iter) {\rdecision \u0026lt;- \u0026quot;Test reached the max interation\u0026quot;\ris_reject_null \u0026lt;- FALSE\r}\r} return(list(\rstopped_n = n,\rdecision = decision,\ris_reject_null = is_reject_null\r))\r}\ri. Under the null (\\(H_0: p = 0.5\\))\rset.seed(1)\r# Under the null p_true = 0.5\r# LRT (binomial)\rbinom_null_err \u0026lt;- mean(replicate(max_iter, {\rrun_binom_test(n, p_true = p0,\rp0, p1,\ralpha)\r}))\rprintf(\u0026quot;Type 1 error of LRT (n = %i) is %.2f\u0026quot;, n, binom_null_err)\r## [1] \u0026quot;Type 1 error of LRT (n = 280) is 0.05\u0026quot;\r# SPRT sprt_null_out \u0026lt;- replicate(max_iter, {\rout \u0026lt;- run_wald_sprt(p_true = p0, p0, p1,\ralpha, beta, max_iter = 1e+3L)\rreturn(c(stopped_n = out$stopped_n, is_reject_null = out$is_reject_null))\r})\rsprt_null_err \u0026lt;- mean(sprt_null_out[\u0026quot;is_reject_null\u0026quot;, ])\rsprt_null_sample_size \u0026lt;- mean(sprt_null_out[\u0026quot;stopped_n\u0026quot;, ])\rprintf(\u0026quot;Type 1 error of SPRT (E[N] = %.1f) is %.2f\u0026quot;, sprt_null_sample_size, sprt_null_err)\r## [1] \u0026quot;Type 1 error of SPRT (E[N] = 137.7) is 0.04\u0026quot;\rUnder the null, both LRT and SPRT control type-1 error, and SPRT has a smaller average sample size compared to the LRT.\n\rii. Under the alternative (\\(H_1: p = 0.6\\))\rset.seed(1)\r# Under the alternative p_true = 0.6\r# LRT (binomial)\rbinom_power \u0026lt;- mean(replicate(max_iter, {\rrun_binom_test(n, p_true = p1,\rp0, p1,\ralpha)\r}))\rprintf(\u0026quot;Power of LRT (n = %i) is %.2f\u0026quot;, n, binom_power)\r## [1] \u0026quot;Power of LRT (n = 280) is 0.96\u0026quot;\r# SPRT\rsprt_power_out \u0026lt;- replicate(max_iter, {\rout \u0026lt;- run_wald_sprt(p_true = p1, p0, p1,\ralpha, beta, max_iter = 1e+3L)\rreturn(c(stopped_n = out$stopped_n, is_reject_null = out$is_reject_null))\r})\rsprt_power \u0026lt;- mean(sprt_power_out[\u0026quot;is_reject_null\u0026quot;, ])\rsprt_power_sample_size \u0026lt;- mean(sprt_power_out[\u0026quot;stopped_n\u0026quot;, ])\rprintf(\u0026quot;Power of SPRT (E[N] = %.1f) is %.2f\u0026quot;, sprt_power_sample_size, sprt_power)\r## [1] \u0026quot;Power of SPRT (E[N] = 139.3) is 0.96\u0026quot;\rUnder the alternative, both LRT and SPRT achieve the minimum power and SPRT has a smaller average sample size compared to the LRT.\n\riii. Under a misspecified alternative 1. small \\(p = 0.55\\).\rset.seed(1)\r# Under a misspecified alternative 1. small p_true = 0.55.\rp_mis1 \u0026lt;- 0.55\r# LRT (binomial)\rbinom_power_mis1 \u0026lt;- mean(replicate(max_iter, {\rrun_binom_test(n, p_true = p_mis1,\rp0, p1,\ralpha)\r}))\rprintf(\u0026quot;Power of LRT (n = %i) is %.2f\u0026quot;, n, binom_power_mis1)\r## [1] \u0026quot;Power of LRT (n = 280) is 0.52\u0026quot;\r# SPRT\rsprt_power_mis1_out \u0026lt;- replicate(max_iter, {\rout \u0026lt;- run_wald_sprt(p_true = p_mis1, p0, p1,\ralpha, beta, max_iter = 1e+3L)\rreturn(c(stopped_n = out$stopped_n, is_reject_null = out$is_reject_null))\r})\rsprt_power_mis1 \u0026lt;- mean(sprt_power_mis1_out[\u0026quot;is_reject_null\u0026quot;, ])\rsprt_power_mis_1_sample_size \u0026lt;- mean(sprt_power_mis1_out[\u0026quot;stopped_n\u0026quot;, ])\rprintf(\u0026quot;Power of SPRT (E[N] = %.1f) is %.2f\u0026quot;, sprt_power_mis_1_sample_size, sprt_power_mis1)\r## [1] \u0026quot;Power of SPRT (E[N] = 234.5) is 0.50\u0026quot;\rUnder a misspecified alternative with \\(p = 0.55 \u0026lt; p_1 = 0.6\\), both LRT and SPRT achieve a similar power but SPRT has a smaller average sample size. The achieved power is below the pre-specified minimum power as the true success probability is smaller than the alternative.\n\riv. Under a misspecified alternative 2. large \\(p = 0.65\\).\rset.seed(1)\r# Under a misspecified alternative 2. large p_true = 0.65.\rp_mis2 \u0026lt;- 0.65\r# LRT (binomial)\rbinom_power_mis2 \u0026lt;- mean(replicate(max_iter, {\rrun_binom_test(n, p_true = p_mis2,\rp0, p1,\ralpha)\r}))\rprintf(\u0026quot;Power of LRT (n = %i) is %.2f\u0026quot;, n, binom_power_mis2)\r## [1] \u0026quot;Power of LRT (n = 280) is 1.00\u0026quot;\rsprt_power_mis2_out \u0026lt;- replicate(max_iter, {\rout \u0026lt;- run_wald_sprt(p_true = p_mis2, p0, p1,\ralpha, beta, max_iter = 1e+3L)\rreturn(c(stopped_n = out$stopped_n, is_reject_null = out$is_reject_null))\r})\rsprt_power_mis2 \u0026lt;- mean(sprt_power_mis2_out[\u0026quot;is_reject_null\u0026quot;, ])\rsprt_power_mis_2_sample_size \u0026lt;- mean(sprt_power_mis2_out[\u0026quot;stopped_n\u0026quot;, ])\rprintf(\u0026quot;Power of SPRT (E[N] = %.1f) is %.2f\u0026quot;, sprt_power_mis_2_sample_size, sprt_power_mis2)\r## [1] \u0026quot;Power of SPRT (E[N] = 76.1) is 1.00\u0026quot;\rUnder a misspecified alternative with \\(p = 0.65 \u0026gt; p_1 = 0.6\\), both LRT and SPRT have almost a power of 1 but SPRT has a much smaller average sample size as SPRT can early stop the procedure adaptively to the underlying higher success probability than the alternative.\n\r\rConclusion\rSequential hypothesis testing procedures can achieve a better sample efficiency even compared to the best-fixed sample size test. In the following post, we will explain the flexibility and safety of sequential testing procedures.\n\r\r\rSince \\(\\bar{X}_n\\) is discrete, there might be no such constant \\(c_\\alpha\\) satisfying the equality. In this case, we can randomized the test, which is beyond the scope of this post.↩︎\n\rComputing the critical value or the corresponding p-value could be computationally expensive if the sample size is large. In this case, we can use a normal approximation-based z-test instead of the exact Binomial test.↩︎\n\rThis is not a free lunch - in worst scenarios, SPRT can reach the max iteration which could be much larger than the fixed sample size of LRT.↩︎\n\rWe can use a mixture of SPRTs to achieve a near optimal sample efficiency for a composite alternative hypothesis space in which a range of alternative distributions exist. This topic has been discussed in literature. For example, see arXiv.↩︎\n\r\r\r","date":1654214400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1654214400,"objectID":"7b1f71809dd1f9797e32c713eba1e3cc","permalink":"https://shinjaehyeok.github.io/post/statistics/sequential_test_efficiency/stcd-tutorial/","publishdate":"2022-06-03T00:00:00Z","relpermalink":"/post/statistics/sequential_test_efficiency/stcd-tutorial/","section":"post","summary":"In this and a follow-up posts, we explain two main advantages of sequential hypothesis testing methods compared to standard tests based on fixed sample size. Sample efficiency in practice As","tags":["ab test","inference","sequential test"],"title":"Advantages of Sequential Hypothesis Testing: 1. Sample efficiency","type":"post"}]