[{"authors":null,"categories":null,"content":"I am a Data Scientist at Google. Previously, I was a postdoctoral researcher in the Department of Statistics at Carnegie Mellon University, working with Aaditya Ramdas and Alessandro Rinaldo. I recently graduated from the same department with a Ph.D. in Statistics.\nResearch My research interest lies in understanding the sequential and adaptive nature of data analysis. I study how commonly used statistical inference procedures behave under the presence of an analyst\u0026rsquo;s data-dependent choices. My current projects focus on designing and analyzing nonasymptotic sequential testings and online change-point detection procedures.\nDuring the Ph.D., my thesis\u0026rsquo;s focus was to qualitatively and quantitatively measure the selection bias caused by different sorts of human interventions in settings of adaptive experimentation like multi-armed bandits, advised by Aaditya Ramdas and Alessandro Rinaldo. I have also worked extensively with Larry Wasserman to understand the geometrical and topological structure of data-generating processes.\nKeywords: Anytime-valid inference, Sequential test, Multi-armed bandit, Change-point detection\nRecent Papers E-detectors: a nonparametric framework for online changepoint detection\nJ. Shin, A. Ramdas, A. Rinaldo\nPreprint arXiv\nNonparametric iterated-logarithm extensions of the sequential generalized likelihood ratio test\nJ. Shin, A. Ramdas, A. Rinaldo\nIEEE Journal on Selected Areas in Information Theory arXiv, code\nOn conditional versus marginal bias in multi-armed bandits\nJ. Shin, A. Ramdas, A. Rinaldo\nThirty-seventh International Conference on Machine Learning (ICML 2020) arXiv\nAre sample means in multi-armed bandits positively or negatively biased?\nJ. Shin, A. Ramdas, A. Rinaldo\nNeural Information Processing Systems (NeurIPS 2019, Spotlight) arXiv\nOn the bias, risk and consistency of sample means in multi-armed bandits\nJ. Shin, A. Ramdas, A. Rinaldo\nSIAM Journal on Mathematics of Data Science arXiv proc\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://shinjaehyeok.github.io/author/jaehyeok-shin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/jaehyeok-shin/","section":"authors","summary":"I am a Data Scientist at Google. Previously, I was a postdoctoral researcher in the Department of Statistics at Carnegie Mellon University, working with Aaditya Ramdas and Alessandro Rinaldo. I recently graduated from the same department with a Ph.","tags":null,"title":"Jaehyeok Shin","type":"authors"},{"authors":[],"categories":["R package"],"content":" SGLRT SGLRT is a R package implementation of Sequential Generalized Likelihood Ratio (GLR)-like Tests and confidence sequences in\nNonparametric iterated-logarithm extensions to Lorden’s treatment of the sequential GLRT\nJ. Shin, A. Ramdas, A. Rinaldo arXiv\nInstallation You can install the SGLRT package from GitHub with:\n# install.packages(\u0026quot;devtools\u0026quot;) devtools::install_github(\u0026quot;shinjaehyeok/SGLRT_public\u0026quot;)  How to reproduce all plots and simulation results in SRR’20. To reproduce plots, you need to install latex2exp package which parses and converts LaTeX math formulas to R’s plotmath expressions. It is is not installed, you can run the following command to install it.\ninstall.packages(\u0026quot;latex2exp\u0026quot;) 1. Compare boundaries for sequential GLR tests. (Fig. 3 in Section III-A) For a given exponential family distribution, let \\(\\mathrm{GLR}_n (\u0026gt; \\mu_1, \\leq\\mu_0)\\) be the GLR statistic based \\(n\\) i.i.d. observations for the one-sided testing problem: \\[ H_0 : \\mu \\leq \\mu_0~~\\text{vs}~~H_1 : \\mu \u0026gt;\\mu_1, \\] for some \\(\\mu_0 \u0026lt; \\mu_0\\) in the space of mean parameters. For any constant \\(g \u0026gt; 0\\), Lorden 1973 proved the following upper bound on the boundary-crossing probability:1 \\[ \\sup_{\\mu \\leq \\mu_0}\\mathbb{P}_{\\mu} \\left(n \\geq 1: \\log\\mathrm{GLR}_n(\u0026gt;\\mu_1,\\leq\\mu_0) \\geq g\\right) \\leq \\begin{cases} e^{-g} \u0026amp;\\mbox{if } \\mathrm{KL}(\\mu_1,\\mu_0) \\geq g \\\\ \\left(1 + \\frac{g}{\\mathrm{KL}(\\mu_1,\\mu_0)} \\right)e^{-g} \u0026amp; \\mbox{otherwise }. \\end{cases} \\]\nAlternatively, in our paper, we proved the following inequality holds:\n\\[ \\sup_{\\mu \\leq \\mu_0}\\mathbb{P}_{\\mu} \\left(\\exists n \\geq 1: \\log\\mathrm{GLR}_n(\u0026gt;\\mu_1,\\leq\\mu_0) \\geq g\\right) \\leq \\begin{cases} e^{-g} \u0026amp;\\mbox{if } \\mathrm{KL}(\\mu_1,\\mu_0) \\geq g \\\\ \\inf_{\\eta \u0026gt;1} \\left\\lceil \\log_\\eta \\left(\\frac{g}{\\mathrm{KL}(\\mu_1,\\mu_0)}\\right)\\right\\rceil e^{-g / \\eta} \u0026amp; \\mbox{otherwise }. \\end{cases} \\] (In the paper, a nonparametric generalization of the above inequality was presented.)\nNow, for any \\(\\alpha \\in (0,1)\\), let \\(g_\\alpha^L(\\mu_1,\\mu_0)\\) and \\(g_\\alpha(\\mu_1,\\mu_0)\\) be smallest boundary values which make RHS of two above inequalities equal to \\(\\alpha\\), respectively. The following Rcode reproduces the Fig. 3 in Section III-A in which we compared \\(g_\\alpha^L(\\mu_1,\\mu_0)\\) and \\(g_\\alpha(\\mu_1,\\mu_0)\\) for normal distributions with \\(\\sigma = 1\\). (In this case, \\(\\mathrm{KL}(\\mu_1 ,\\mu_0) = (\\mu_1 - \\mu_0)^2 / 2\\).)\nlibrary(latex2exp) library(SGLRT) alpha \u0026lt;- 10^seq(-1,-10) theta_vec \u0026lt;- c(1,0.5, 10^seq(-1,-10)) theta_vec \u0026lt;- 2^seq(0,-30) d_vec \u0026lt;- theta_vec^2 / 2 for (i in seq_along(alpha[1:3])){ f_lorden \u0026lt;- function(d) const_boundary_lorden(alpha[i], d) lorden \u0026lt;- sapply(d_vec, f_lorden) f_ours \u0026lt;- function(d) const_boundary(alpha[i], d) ours \u0026lt;- sapply(d_vec, f_ours) if (i == 1){ plot(1/theta_vec, unlist(lorden[\u0026quot;g\u0026quot;,]), type = \u0026quot;l\u0026quot;, log =\u0026quot;x\u0026quot;, ylab = \u0026quot;Boundary Value\u0026quot;, xlab = TeX(\u0026quot;$|\\\\mu_1 - \\\\mu_0|^{-1}$ (log scale)\u0026quot;)) points(1/theta_vec, unlist(ours[\u0026quot;g\u0026quot;,]), type = \u0026quot;l\u0026quot;, col = 2) } else { points(1/theta_vec, unlist(lorden[\u0026quot;g\u0026quot;,]), type = \u0026quot;l\u0026quot;, col = 1, lty = i) points(1/theta_vec, unlist(ours[\u0026quot;g\u0026quot;,]), type = \u0026quot;l\u0026quot;, col = 2, lty = i) } } legend(\u0026quot;topleft\u0026quot;, TeX(c(paste0(\u0026quot;Lorden\u0026#39;s ($\\\\alpha = \u0026quot;, alpha[1:3],\u0026quot;$)\u0026quot;), paste0(\u0026quot;Ours ($\\\\alpha = \u0026quot;, alpha[1:3],\u0026quot;$)\u0026quot;))), col = c(rep(1,3), rep(2,3)), lty = rep(1:3,2))   2. Ratio of CI’s width to CLT (Fig. 5 in Section IV-C) For each \\(n\\), let \\(\\mathrm{CI}_n(\\alpha)\\) be a random set based on first \\(n\\) observations \\(X_1, \\dots, X_n\\) from a distribution with mean \\(\\mu\\). If the sequence of random sets \\(\\{\\mathrm{CI}_n(\\alpha)\\}_{n \\in \\mathbb{N}}\\) satisfies the following inequality: \\[ \\mathbb{P}_\\mu \\left( \\mu \\in \\mathrm{CI}_n(\\alpha), \\forall n \\in \\mathbb{N}\\right) \\geq 1-\\alpha,~~\\forall \\mu \\in (0,1), \\] it is called a confidence sequence with confidence level \\(\\alpha\\) (e.g, see Howard et al., 20182 and references therein for a comprehensive summary of related literature.)\nBased on the upper bound on the boundary crossing probability above, in our paper, we presented two novel confidence sequences called GLR-like and discrete mixture based confidence sequences. For sub-Gaussian distributions with parameter \\(\\sigma\\), The GLR-like confidence sequence is given by \\[ \\mathrm{CI}_n^{\\mathrm{G}} := \\begin{cases} \\left(\\bar{X}_n - \\sigma\\sqrt{\\frac{g_\\alpha}{n_{\\min}}}\\left[ \\sqrt{2} + \\frac{1}{\\sqrt{2}}\\left(\\frac{n_{\\min}}{n}-1\\right)\\right], \\infty\\right) \u0026amp;\\mbox{if } n \\in [n_0,n_{\\min}) \\\\ \\left( \\bar{X}_n - \\sigma\\sqrt{\\frac{2g_\\alpha}{n}}, \\infty\\right) \u0026amp;\\mbox{if } n \\in [n_{\\min}, n_{\\max}] \\\\ \\left(\\bar{X}_n - \\sigma\\sqrt{\\frac{g_\\alpha}{n_{\\max}}}\\left[ \\sqrt{2} + \\frac{1}{\\sqrt{2}}\\left(\\frac{n_{\\max}}{n}-1\\right)\\right], \\infty\\right) \u0026amp;\\mbox{if } n \\in (n_{\\max}, \\infty) \\end{cases}, \\] for any given level \\(\\alpha \\in (0, 1)\\) and target time interval \\([n_{\\min}, n_{\\max}]\\) on which the confidence sequence is time-uniformly close to the Chrenoff bound. Unlike the GLR-like one, the discrete mixture based confidence sequence \\(\\{\\mathrm{CI}_n^{\\mathrm{DM}}\\}\\) does not have an explicit form and see the paper for the detailed explanation how to compute \\(\\mathrm{CI}_n^{\\mathrm{DM}}\\) for given sample mean \\(\\bar{X}_n\\) and confidence level \\(\\alpha\\). By the construction, we have \\(\\mathrm{CI}_n^{\\mathrm{G}} \\subset \\mathrm{CI}_n^{\\mathrm{DM}}\\).\nIn Section IV-C of the paper, we compare these bounds with the stitching and normal mixture bounds in Howard et al., 2018 where each confidence intervals for stitching \\(\\mathrm{CI}_{n}^{\\mathrm{ST}}\\) and normal mixture method \\(\\mathrm{CI}_n^{\\mathrm{NM}}\\) is given by \\[ \\begin{aligned} \\mathrm{CI}_{n}^{\\mathrm{ST}} \u0026amp;:= \\left(\\bar{X}_n- \\frac{1.7}{\\sqrt{n}}\\sqrt{\\log\\log(2n) + 0.72 \\log\\left(\\frac{5.2}{\\alpha}\\right)}, \\infty \\right) \\\\ \\mathrm{CI}_{n}^{\\mathrm{NM}} \u0026amp;:= \\left(\\bar{X}_n- \\sqrt{2 \\left(1 + \\frac{\\rho}{n}\\right) \\log\\left(\\frac{1}{2\\alpha} \\sqrt{\\frac{n + \\rho}{\\rho} + 1}\\right)}, \\infty \\right), \\end{aligned} \\] where we set \\(\\rho = 1260\\) by following the setting in Figure 9 of Howard et al., 2018.\nThe following Rcode reproduces the Fig. 5 in Section IV-C in which we compared ratios of widths of confidence intervals above to the pointwise and asymptotically valid normal confidence intervals based on the central limit theorem.\nlibrary(SGLRT) # Chernoff chornoff \u0026lt;- function(v, alpha = 0.025){ sqrt(2 * log(1/alpha) / v) } # Normal mixture function normal_mix \u0026lt;- function(v, alpha = 0.025, rho = 1260){ sqrt(2 * (1 + rho / v) * log(1/(2*alpha) * sqrt((v + rho)/rho) + 1)) } #Stitching stitch \u0026lt;- function(v, alpha = 0.025){ 1.7 * sqrt((log(log(2) + log(v)) + 0.72 * log(5.2/alpha)) / v) } # CLT CLT \u0026lt;- function(v, alpha = 0.025){ qnorm(1-alpha) / sqrt(v) } # Functions to construct our bounds # First one has target interval [1, nmax] # Second one has target interval [nmax / 20, nmax * 4] alpha \u0026lt;- 0.025 nmax \u0026lt;- 1e+5 nmax1 \u0026lt;- nmax nmin1 = 1 nmax2 \u0026lt;- nmax1 * 4 nmin2 \u0026lt;- nmax1 / 20 ours1 \u0026lt;- SGLR_CI_additive(alpha, nmax1, nmin1) ours2 \u0026lt;- SGLR_CI_additive(alpha, nmax2, nmin2) # Compute the width of CIs on exponentially-spaced grid. M \u0026lt;- log(nmax, base = 10) v \u0026lt;- 10^(seq(0,M + 0.2, length.out = 100)) # Compute existing bounds chornoff_vec \u0026lt;- sapply(v, chornoff) CLT_vec \u0026lt;- sapply(v, CLT) normal_mix_vec \u0026lt;- sapply(v, normal_mix) stit_vec \u0026lt;- sapply(v, stitch) existing_list \u0026lt;- list(stitch = stit_vec, normal_mix = normal_mix_vec) # Compute our bounds GLR_like_1_vec \u0026lt;- sapply(v, ours1$GLR_like_fn) GLR_like_2_vec \u0026lt;- sapply(v, ours2$GLR_like_fn) our_dis_mix_1_vec \u0026lt;- sapply(v, ours1$dis_mix_fn) our_dis_mix_2_vec \u0026lt;- sapply(v, ours2$dis_mix_fn) ours_list_1 \u0026lt;- list(GLR_like_1 = GLR_like_1_vec, our_dis_mix_1 = our_dis_mix_1_vec) ours_list_2 \u0026lt;- list(GLR_like_2 = GLR_like_2_vec, our_dis_mix_2 = our_dis_mix_2_vec) # Plot ratio of bounds title \u0026lt;- \u0026quot;Ratio of CI\u0026#39;s width to CLT\u0026quot; plot(v, chornoff_vec / CLT_vec, type = \u0026quot;l\u0026quot;, main = title, ylab = \u0026quot;Ratio\u0026quot;, xlab = \u0026quot;n\u0026quot;, ylim = c(1, 4), xlim = c(1, nmax)) col = 1 legend_col \u0026lt;- c(1) for (i in seq_along(existing_list)){ col \u0026lt;- col + 1 legend_col \u0026lt;- c(legend_col, col) lines(v, existing_list[[i]] / CLT_vec, col = col) } legend_lty \u0026lt;- rep(1, length(existing_list) + 1) for (i in seq_along(ours_list_1)){ col \u0026lt;- col + 1 legend_col \u0026lt;- c(legend_col, col) lines(v, ours_list_1[[i]] / CLT_vec, lty = 2, lwd = 2, col = col) } legend_lty \u0026lt;- c(legend_lty, rep(2, length(ours_list_1))) for (i in seq_along(ours_list_2)){ col \u0026lt;- col + 1 legend_col \u0026lt;- c(legend_col, col) lines(v, ours_list_2[[i]] / CLT_vec, lty = 4, lwd = 2, col = col) } legend_lty \u0026lt;- c(legend_lty, rep(4, length(ours_list_2))) abline(v = c(nmin1, nmin2, nmax1, nmax2), lty = 3) bounds_name \u0026lt;- c(\u0026quot;Chernoff\u0026quot;, \u0026quot;Stitching (HRMS\u0026#39;20)\u0026quot;, \u0026quot;Normal Mix. (HRMS\u0026#39;20)\u0026quot;, \u0026quot;GLR-like 1 (Ours)\u0026quot;, \u0026quot;Discrete Mix. 1 (Ours)\u0026quot;, \u0026quot;GLR-like 2 (Ours)\u0026quot;, \u0026quot;Discrete Mix. 2 (Ours)\u0026quot;) legend(\u0026quot;topright\u0026quot;, bounds_name, lty = legend_lty, col = legend_col, bg= \u0026quot;white\u0026quot;)   3. Efficiency of GLR-like and discrete mixture tests (Table I-VI in Appendix D) The following Rcode reproduces Table I-VI in Appendix D in which we compared performances of sequential GLR-like and Discrete mixture tests to standard fixed sample size based tests for Gaussian and Bernoulli observations. In this document, we repeated the simulation only 100 times to save the computation time but, in the paper, the simulation result based on 2000 times repetition is presented.\n# Type 1 and Type 2 error control simulation library(SGLRT) # Define functions for tests based on a fixed sample size. # Compute power of Z-test G_pwr \u0026lt;- function(n, mu_target, mu_0, alpha){ thres_exact \u0026lt;- qnorm(alpha, mu_0, sd = 1/sqrt(n), lower.tail = FALSE) pwr \u0026lt;- pnorm(thres_exact, mu_target, sd = 1/sqrt(n), lower.tail = FALSE) return(pwr) } # Z-test function (reject the null if the returned value is positive) z_test \u0026lt;- function(x_bar, n, mu_0, alpha){ z_alpha \u0026lt;- qnorm(1-alpha) x_bar - mu_0 - z_alpha / sqrt(n) } # Compute power of binomial test binom_pwr \u0026lt;- function(n, mu_target, mu_0, alpha){ n \u0026lt;- floor(n) thres_exact \u0026lt;- qbinom(alpha, n, mu_0, lower.tail = FALSE) pwr \u0026lt;- pbinom(thres_exact, n, mu_target, lower.tail = FALSE) return(pwr) } # Exact binomial test (reject the null if the returned value is positive) binom_test \u0026lt;- function(x_bar, n, mu_0, alpha){ thres_exact \u0026lt;- qbinom(alpha, n, mu_0, lower.tail = FALSE) return(x_bar * n - thres_exact) } # Sequential tests # GLR-like and discrete mixture test for sub-Gaussian seq_G_test_generator \u0026lt;- function(alpha, nmax, nmin){ G_out \u0026lt;- SGLR_CI(alpha, nmax, nmin) return(G_out) } # GLR-like and discrete mixture test for sub-Bernoulli ber_fn_list \u0026lt;- generate_sub_ber_fn() seq_ber_test_generator \u0026lt;- function(alpha, nmax, nmin){ ber_out \u0026lt;- SGLR_CI(alpha, nmax, nmin, breg = ber_fn_list$breg, breg_pos_inv = ber_fn_list$breg_pos_inv, breg_neg_inv = ber_fn_list$breg_neg_inv, breg_derv = ber_fn_list$breg_derv, mu_lower = ber_fn_list$mu_lower, mu_upper = ber_fn_list$mu_upper, grid_by = ber_fn_list$grid_by) return(ber_out) } # Sample generators # Gaussian samples G_sample \u0026lt;- function(n, mu_true){ rnorm(n, mean = mu_true) } # Bernoulli samples ber_sample \u0026lt;- function(n, mu_true){ rbinom(n, 1, prob = mu_true) } # Function to summarize the simulation result summ_simul \u0026lt;- function(result_name, simul_out, mu_true_vec){ out \u0026lt;- data.frame(mu_true = mu_true_vec, exact_hacking = numeric(length(mu_true_vec)), GLR_like = numeric(length(mu_true_vec)), dis_mix = numeric(length(mu_true_vec)), exact_test = numeric(length(mu_true_vec))) for (i in seq_along(mu_true_vec)){ out[i, -1] \u0026lt;- simul_out[[i]][[result_name]] } return(out) }  3-1. Gaussian case In the Gaussian setting, we compared sequential GLL-like and Discrete mixture tests to the Z-test. We set the null hypothesis as the mean of the Gaussian distribution is less than or equal to 0, and the alternative hypothesis as the true mean is greater than 0.1. The Z-test was performed based on a fixed sample size to make the test control both type-1 and type-2 errors by 0.1. In the following three tables, results of all three testing procedures are summarized. In the first table, we show that, for each underlying true mean, how frequently each testing procedure rejects the null hypothesis. Here the column Z-test (p-hacking) represents the naive usage of Z-test as a sequential procedure in which we stop and reject the null whenever p-value of the Z-test goes below the level 0.1. This is an example of p-hacking which inflates the type-1 error significantly larger than the target level. From the first table, we can check that the Z-test with continuous monitoring of p-values yields a large type 1 error (0.43) even under a null distribution (mean = -0.5) which is a way from the boundary of the null. In contrast, for all other testing procedures, type 1 errors are controlled under the null distributions.\nFor alternative distributions (mean \u0026gt; 0.1), the Z-test has larger powers than the prespecified bound (0.9) as expected. Note that the discrete mixture based sequential test achieves almost the same power compared to the Z-test. However, as we can check from the second and third tables, under the alternative distributions, the discrete mixture test detects the signal faster than the Z-test both on average and with high probability. The GLR-like test yields a weaker power at the boundary of the alternative space but it achieves higher powers and smaller sample size as the underlying true means being farther away from the boundary.\nHowever, GLR-like and discrete mixture tests do not always perform better than the Z-test. If the underlying true mean lies between boundaries of null and alternative spaces, both test have weaker power and requires more samples to detect the signal compared to the Z-test. Therefore, we recommend to set the boundary of the alternative conservatively in practice.\n# Gaussian simulation set.seed(1) f_simul \u0026lt;- function(mu) { out \u0026lt;- test_simul(mu_target = 0.1, mu_true = mu, mu_0 = 0, alpha = 0.1, beta = 0.1, B = 100, print_progress = FALSE, print_result = FALSE, sample_generator = G_sample, power_fn = G_pwr, fixed_test_fn = z_test, seq_test_fn = seq_G_test_generator) return(out) } mu_true_vec \u0026lt;- seq(-0.05, 0.2, by = 0.05) simul_G_out \u0026lt;- lapply(mu_true_vec, f_simul) reject_rate_G \u0026lt;- summ_simul(\u0026quot;reject_rate\u0026quot;, simul_G_out, mu_true_vec) sample_size_G \u0026lt;- summ_simul(\u0026quot;sample_size\u0026quot;, simul_G_out, mu_true_vec) early_stop_ratio_G \u0026lt;- summ_simul(\u0026quot;early_stop_ratio\u0026quot;, simul_G_out, mu_true_vec) Estimated probabilities of rejecting the null hypothesis. (Gaussian)      mu_true   exact_hacking   GLR_like   dis_mix   exact_test    1  -0.05  0.48  0.00  0.05  0.01    2  0.00  0.60  0.02  0.07  0.08    3  0.05  0.91  0.11  0.44  0.52    4  0.10  1.00  0.69  0.88  0.86    5  0.15  1.00  0.99  1.00  1.00    6  0.20  1.00  1.00  1.00  1.00    Estimated average sample sizes of testing procedures. (Gaussian)      mu_true   GLR_like   dis_mix   exact_test    1  -0.05  1314.00  1252.01  657.00    2  0.00  1300.92  1241.63  657.00    3  0.05  1240.67  951.47  657.00    4  0.10  897.85  473.77  657.00    5  0.15  506.40  240.82  657.00    6  0.20  272.49  114.01  657.00    Estimated probabilities of tests being stopped earlier than Z-test.      mu_true   GLR_like   dis_mix    1  -0.05  0.00  0.05    2  0.00  0.01  0.06    3  0.05  0.06  0.33    4  0.10  0.30  0.71    5  0.15  0.71  0.96    6  0.20  0.97  1.00     3-2. Bernoulli case In the Bernoulli setting, we compared sequential GLL-like and Discrete mixture tests to the exact binomial test. We set the null hypothesis as the mean of the Bernoulli distribution is less than or equal to 0.1, and the alternative hypothesis as the true mean is greater than 0.12. The exact binomial test was performed based on a fixed sample size to make the test control both type-1 and type-2 errors by 0.1. The following three tables summarize the simulation result. In all three tables, we can check the same pattern we observed from the Gaussian case. In the first table, we show that, for each underlying true mean, how frequently each testing procedure rejects the null hypothesis. Here the column Exact binomial (p-hacking) represents the naive usage of the exact test as a sequential procedure in which we stop and reject the null whenever p-value of the test goes below the level 0.1. As we observed before, the p-hacking inflates the type-1 error significantly larger than the target level. From the table, we can check that the exact binomial test with continuous monitoring of p-values yields a large type 1 error (0.23) even under a null distribution (mean = 0.09) which is a way from the boundary of the null. In contrast, for all other testing procedures, type 1 errors are controlled under the null distributions.\nFor alternative distributions (mean \u0026gt; 0.12), the exact binomial test has larger powers than the prespecified bound 0.9 as expected. Again, as same as the Gaussian case, the discrete mixture based sequential test achieves almost the same power compared to the exact binomial test. However, as we can check from the second and third tables, under the alternative distributions, the discrete mixture test uses, on average and with a high probability, smaller numbers of samples to detect the signal than the exact binomial test with a fixed sample size. The GLR-like test yields a weaker power at the boundary of the alternative space but it achieves higher powers and smaller sample size as the underlying true means being farther away from the boundary.\nHowever, as same as the Gaussian case, GLR-like and discrete mixture tests do not always perform better than the exact binomial test. If the underlying true mean lies between boundaries of null and alternative spaces, both test have weaker power and requires more samples to detect the signal compared to the exact binomial. Therefore, it is recommended to set the boundary of the alternative close to the boundary of the null in practice.\n# Bernoulli simulation set.seed(1) f_simul \u0026lt;- function(mu) { out \u0026lt;- test_simul(mu_target = 0.12, mu_true = mu, mu_0 = 0.1, alpha = 0.1, beta = 0.1, B = 100, print_progress = FALSE, print_result = FALSE, sample_generator = ber_sample, power_fn = binom_pwr, fixed_test_fn = binom_test, seq_test_fn = seq_ber_test_generator) return(out) } mu_true_vec \u0026lt;- seq(0.09, 0.14, by = 0.01) simul_ber_out \u0026lt;- lapply(mu_true_vec, f_simul) reject_rate_ber \u0026lt;- summ_simul(\u0026quot;reject_rate\u0026quot;, simul_ber_out, mu_true_vec) sample_size_ber \u0026lt;- summ_simul(\u0026quot;sample_size\u0026quot;, simul_ber_out, mu_true_vec) early_stop_ratio_ber \u0026lt;- summ_simul(\u0026quot;early_stop_ratio\u0026quot;, simul_ber_out, mu_true_vec) Estimated probabilities of rejecting the null hypothesis. (Bernoulli)      mu_true   exact_hacking   GLR_like   dis_mix   exact_test    1  0.09  0.39  0.00  0.01  0.02    2  0.10  0.62  0.01  0.08  0.09    3  0.11  0.94  0.08  0.44  0.51    4  0.12  0.99  0.63  0.90  0.88    5  0.13  1.00  0.99  1.00  0.99    6  0.14  1.00  1.00  1.00  1.00    Estimated average sample sizes of testing procedures. (Bernoulli)      mu_true   GLR_like   dis_mix   exact_test    1  0.09  3290.00  3257.57  1645.00    2  0.10  3267.50  3065.64  1645.00    3  0.11  3158.03  2299.34  1645.00    4  0.12  2349.01  1157.50  1645.00    5  0.13  1299.54  610.78  1645.00    6  0.14  677.04  292.32  1645.00    Estimated probabilities of tests being stopped earlier than the exact binomial test.      mu_true   GLR_like   dis_mix    1  0.09  0.00  0.01    2  0.10  0.01  0.07    3  0.11  0.04  0.34    4  0.12  0.24  0.73    5  0.13  0.72  0.94    6  0.14  0.98  1.00       Reference 1: G. Lorden, “Open-ended tests for Koopman-Darmois families,”The Annals of Statistics, vol. 1, no. 4, pp. 633–643, 1973. return\n2: S. R. Howard, A. Ramdas, J. McAuliffe, and J. Sekhon, “Uniform, nonparametric, non-asymptotic confidence sequences,”arXiv preprint arXiv:1810.08240, 2018. return\n  ","date":1601164800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601260392,"objectID":"ddd76a45055dfafa68b1fb354359a90d","permalink":"https://shinjaehyeok.github.io/post/sglrt/sglrt-package/","publishdate":"2020-09-27T00:00:00Z","relpermalink":"/post/sglrt/sglrt-package/","section":"post","summary":"SGLRT SGLRT is a R package implementation of Sequential Generalized Likelihood Ratio (GLR)-like Tests and confidence sequences in\nNonparametric iterated-logarithm extensions to Lorden’s treatment of the sequential GLRT\nJ. Shin, A.","tags":["ab test","research"],"title":"SGLRT package ","type":"post"}]