[{"authors":null,"categories":null,"content":"I am a Data Scientist at Google. Previously, I was a postdoctoral researcher in the Department of Statistics at Carnegie Mellon University, working with Aaditya Ramdas and Alessandro Rinaldo. I recently graduated from the same department with a Ph.D. in Statistics.\nResearch My research interest lies in understanding the sequential and adaptive nature of data analysis. I study how commonly used statistical inference procedures behave under the presence of an analyst\u0026rsquo;s data-dependent choices. My current projects focus on designing and analyzing nonasymptotic sequential testings and online change-point detection procedures.\nKeywords: Always-valid inference, Sequential test, Multi-armed bandit, Change-point detection\nRecent Papers E-detectors: a nonparametric framework for online changepoint detection\nJ. Shin, A. Ramdas, A. Rinaldo\nPreprint arXiv, code\nNonparametric iterated-logarithm extensions of the sequential generalized likelihood ratio test\nJ. Shin, A. Ramdas, A. Rinaldo\nIEEE Journal on Selected Areas in Information Theory arXiv, code\nOn conditional versus marginal bias in multi-armed bandits\nJ. Shin, A. Ramdas, A. Rinaldo\nThirty-seventh International Conference on Machine Learning (ICML 2020) arXiv\nAre sample means in multi-armed bandits positively or negatively biased?\nJ. Shin, A. Ramdas, A. Rinaldo\nNeural Information Processing Systems (NeurIPS 2019, Spotlight) arXiv\nOn the bias, risk and consistency of sample means in multi-armed bandits\nJ. Shin, A. Ramdas, A. Rinaldo\nSIAM Journal on Mathematics of Data Science arXiv proc\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://shinjaehyeok.github.io/author/jaehyeok-shin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/jaehyeok-shin/","section":"authors","summary":"I am a Data Scientist at Google. Previously, I was a postdoctoral researcher in the Department of Statistics at Carnegie Mellon University, working with Aaditya Ramdas and Alessandro Rinaldo. I recently graduated from the same department with a Ph.","tags":null,"title":"Jaehyeok Shin","type":"authors"},{"authors":[],"categories":["Statistics"],"content":"\rIf we know when a potential change might possibly happen in a stochastic process then we can set a sequential test to continuously monitor whether we can tell the change is significant or not. For example, let’s imagine we have been playing a series of coin tosses so far. One day, a player brings a new coin. In this case, we know that “today” is the time when a potential change might possibly happen in the fairness of the coin we play. Hence, we can track coin tosses from “today” and tell whether we are still playing with a fair coin or not.\nHowever, what if a player replaced the coin without informing others? The new coin might be still a fair one. But it could be also biased toward head or tail. How can we “detect” a potential change of the underlying stochastic process on the fly without knowing the exact time of the change?\nOnline change detection problem\rThe previous example illustrates a sub field of the statistical analysis called “online change detection problem”. Formally, suppose we have a stream of independent random observations \\(X_1, X_2, \\dots\\). We know that under the normal condition, each observation follows a “pre-change” distribution \\(P\\). But if a change happens at an unknown time \\(\\nu \\geq 0\\) then after the time \\(\\nu\\), each observation in the following sub-sequence \\(X_{\\nu+1}, X_{\\nu+2}, \\dots\\) follows another “post-change” distribution \\(Q\\). Here \\(\\nu = 0\\) implies the change happened at the very beginning so all observations follow the post-change distribution \\(Q\\). On the other hand, we set \\(\\nu = \\infty\\) to indicate a change never happens and all observations follow the pre-change distribution \\(P\\). The coin toss example above can be modeled by \\(P = B(0.5)\\) and \\(Q = B(p)\\) for some \\(p \\neq 0.5\\). For a simple presentation, in this post we assume both pre- and post-change distributions \\(P\\) and \\(Q\\) are known to us in advance. 1 Throughout this post, we refer \\(\\mathbb{E}_{\\nu}\\) to the expectation operator corresponding to the case where the change happened at time \\(\\nu \\in \\{0,1,\\dots,\\infty\\}\\).\nThe standard approach of the online change detection problem is to design a stopping time \\(N\\). If \\(N \u0026lt; \\infty\\) then at the time \\(N\\), we declare a change happened before \\(N\\). One commonly used metric of the efficiency of the stopping time \\(N\\) is the worst average detection delay (WAD) conditioned on the detection time being later than the true change point that is defined by\r\\[\r\\mathcal{J}(N) := \\sup_{v\\geq 0}\\mathbb{E}_\\nu \\left[N - \\nu | N \u0026gt; \\nu\\right].\r\\]\nNote that the WAD is lower bounded by \\(1\\) and this lower bound can be easily achieved by setting \\(N = 1\\). That is, we just declare there was a change as soon as observing a single data point. If there was no change point then we can declare the change after observing another data point and so on.\nOf course, this type of trivial change detector is far from ideal as it will trigger lots of false alerts at the highest possible frequency. In other words, this trivial change detector has the shortest run length, 1, until triggering a false alert. Therefore, when we design a change detector \\(N\\), we want the stopping time \\(N\\) to be able to detect the change as quickly as possible while controlling the false alert rate at a pre-defined rate. A commonly used metric for false alert rate is called the average run length (ARL) defined by the expectation of the stopping time of the change detector under the pre-change distribution. Formally, for a fixed constant \\(\\alpha \\in (0,1)\\), we call a stopping time \\(N\\) contols the ARL by \\(1/\\alpha\\) if\r\\[\r\\mathbb{E}_{\\infty}[N] \\geq 1/\\alpha.\r\\]\nBuilding a change detector based on repeated sequential tests\rSo far, we have discussed how one can formulate the online change detection problem: Find a stopping time \\(N\\) minimizing the worst average detection delay (WAD), \\(\\mathcal{J}(N)\\) while controlling average run length (ARL) by \\(1/\\alpha\\), that is, \\(\\mathbb{E}_{0}[N] \\geq 1/\\alpha.\\) But, how we can construct a such stopping time \\(N\\)? The Lorden’s lemma provides a simple way to convert any sequential testing method into an online change detector by proving a fact that repeated sequential tests can be viewed as an online change detection procedure.2\nLemma 1 (Lorden, G. 1971) Let \\(N\\) be a stopping time of a sequential test controlling type-1 error by \\(\\alpha\\) under the pre-change distribution for a fixed \\(\\alpha \\in (0,1)\\). That is, \\(\\mathbb{P}_{\\infty}\\left(N \u0026lt; \\infty\\right) \\leq \\alpha\\). Now, let \\(N_k\\) denote the stopping time obtained by applying \\(N\\) to \\(X_{k}, X_{k+1}, \\dots\\), and define another stopping time \\(N^*\\) such that\r\\[\rN^* := \\min_{k \\geq 1} \\left\\{N_k + k - 1\\right\\}.\r\\] Then \\(N^*\\) is a change detector controlling ARL by \\(1/\\alpha\\), that is,\r\\[ \\mathbb{E}_{\\infty}\\left[N^*\\right] \\geq 1/\\alpha.\\]\rFurthermore, the WAD of \\(N^*\\) is upper bouned as\r\\[\r\\mathcal{J}(N) := \\sup_{v\\geq 0}\\mathbb{E}_\\nu \\left[N - \\nu | N \u0026gt; \\nu\\right] \\leq \\mathbb{E}_{0}N. \\]\nAs a concrete example, let’s go back to the coin toss example. If we want to test whether a coin is fair (\\(P = B(p),~~p =0.5\\)) or biased toward the head (\\(Q = B(q),~~q \u0026gt;0.5\\)) with a level \\(\\alpha \\in (0,1)\\) then we can use the Wald’s SPRT, which is given by the following stopping time,\r\\[\rN := \\inf\\left\\{n \\geq 1: \\sum_{i=1}^n \\log\\Lambda_i \\geq \\log(1/\\alpha)\\right\\},\r\\]\rwhere each \\(\\Lambda_n\\) is the likelihood ratio of \\(P\\) over \\(Q\\), which is given by\r\\[\r\\Lambda_n := \\left(\\frac{q}{p}\\right)^{X_n}\\left(\\frac{1-q}{1-p}\\right)^{1-X_n},\r\\]\rfor a sequence of Bernoulli observations \\(X_1, X_2, \\dots \\in \\{0,1\\}\\).\nIf the stopping time happen (\\(N \u0026lt; \\infty\\)) then we stop and reject the null hypothesis, so we conclude the coin is biased toward the head. This sequential test controls the type-1 error by \\(\\alpha\\) as we have \\(P(N \u0026lt; \\infty) \\leq \\alpha\\).\nNow, by using the Lorden’s lemma, we can convert this sequential test into a change detection procedure \\(N^*\\), which is given by\r\\[\\begin{align*}\rN^* \u0026amp;:= \\min_{k \\geq 1} \\left\\{N_k + k - 1\\right\\} \\\\\r\u0026amp; = \\inf\\left\\{n \\geq 1: \\max_{k \\geq 1}\\sum_{i=k}^n \\log\\Lambda_i \\geq \\log(1/\\alpha)\\right\\}.\r\\end{align*}\\]\rIf the stopping time \\(N^*\\) happen (\\(N^* \u0026lt; \\infty\\)) then we stop and declare that we detect a change. Note that this change detection procedure is nothing but repeated Wald’s SPRT at each time \\(k\\). From the Lorden’s lemma, this online change detection procedure controls the ARL by \\(1/\\alpha\\), that is, we have \\(\\mathbb{E}_{\\infty}[N^*] \\geq 1/\\alpha\\). In general, we cannot get the exact ARL control \\(\\mathbb{E}_{\\infty}[N^*] = 1/\\alpha\\) by using the Lorden’s lemma. However, this method often performs almost as good as the optimal one. Especially for this Bernoulli case, if we replace the threshold \\(\\log(1/\\alpha)\\) with another constant \\(a\\) achieveing the exact ARL control then this procedure recovers the CUSUM procedrue which is known to be optimal procedure in this example.\nImplementation\rThe change detection procedure \\(N^*\\) can be implemented in an online fashion as follows:\nSet \\(M_0 := 0\\)\nUpdate \\(M_n := \\max\\{M_{n-1}, 0\\} + \\log \\Lambda_n\\)\nMake one of two following decisions:\nIf \\(M_n \\geq \\log(1/\\alpha)\\) then stop and declare a change happened.\rOtherwise, continue to the next iteration.\rCase 1. No change happens (\\(\\nu = \\infty\\))\rset.seed(1)\rn_max \u0026lt;- 500L\r# This memory is not required but only for the simple visualization.\rm_vec \u0026lt;- numeric(n_max)\rm \u0026lt;- 0\r# Unbiased coin toss example\rp_true \u0026lt;- 0.5\r# Set up the baseline likelihood ratio\r# Pre-change: p = 0.5\r# Post-change: q = 0.6 p \u0026lt;- 0.5\rq \u0026lt;- 0.6\r# ARL control at 1000\ralpha \u0026lt;- 1e-3 n_star \u0026lt;- Inf\rnot_yet_detected \u0026lt;- TRUE\rfor (i in 1:n_max) {\r# Observe a new coin toss\rx \u0026lt;- rbinom(1, 1, prob = p_true)\r# Update the change statistic\rm \u0026lt;- max(m, 0) + ifelse(x == 1, log(q/p), log((1-q)/(1-p)))\r# Check whether the stopping time happens\rif (m \u0026gt; log(1/alpha) \u0026amp; not_yet_detected) {\rn_star \u0026lt;- i\rnot_yet_detected \u0026lt;- FALSE\r# For the normal implementation, we can stop at the stopping time\r# break\r}\r# Save the statistic for visualization\rm_vec[i] \u0026lt;- m\r}\rplot(1:n_max, m_vec, type = \u0026quot;l\u0026quot;, xlab = \u0026quot;n\u0026quot;, ylab = expression(\u0026#39;M\u0026#39;[\u0026#39;n\u0026#39;]),\rylim = c(0, 20))\rabline(h = log(1/alpha), col = 2)\rif (!is.infinite(n_star)) {\rabline(v = n_star, col = 2)\rprint(paste0(\u0026quot;Change detected at \u0026quot;, n_star))\r} else {\rprint(\u0026quot;No change detected\u0026quot;)\r}\r## [1] \u0026quot;No change detected\u0026quot;\rIn the first example, we keep using the fair coin and there is no change point (\\(\\nu = \\infty\\)).\rIn this case, the change detection statistic \\(M_n\\) stays below the detection line \\(y = \\log(1/\\alpha)\\), and thus we are correctly not detecting a change (\\(N^* = \\infty\\)).\nCase 2. Change happens at \\(\\nu = 200\\)\rset.seed(1)\rn_max \u0026lt;- 500L\r# This memory is not required but only for the simple visualization.\rm_vec \u0026lt;- numeric(n_max)\rm \u0026lt;- 0\r# Biased coin toss example\rp_pre \u0026lt;- 0.5\rp_post \u0026lt;- 0.6\rnu \u0026lt;- 200\r# Set up the baseline likelihood ratio\r# Pre-change: p = 0.5\r# Post-change: q = 0.6 p \u0026lt;- 0.5\rq \u0026lt;- 0.6\r# ARL control at 1000\ralpha \u0026lt;- 1e-3 n_star \u0026lt;- Inf\rnot_yet_detected \u0026lt;- TRUE\rfor (i in 1:n_max) {\r# Observe a new coin toss\rif (i \u0026lt; nu) {\rx \u0026lt;- rbinom(1, 1, prob = p_pre)\r} else {\rx \u0026lt;- rbinom(1, 1, prob = p_post)\r}\r# Update the change statistic\rm \u0026lt;- max(m, 0) + ifelse(x == 1, log(q/p), log((1-q)/(1-p)))\r# Check whether the stopping time happens\rif (m \u0026gt; log(1/alpha) \u0026amp; not_yet_detected) {\rn_star \u0026lt;- i\rnot_yet_detected \u0026lt;- FALSE\r# For the normal implemenation, we can stop at the stopping time\r# break\r}\r# Save the statistic for visualization\rm_vec[i] \u0026lt;- m\r}\rplot(1:n_max, m_vec, type = \u0026quot;l\u0026quot;, xlab = \u0026quot;n\u0026quot;, ylab = expression(\u0026#39;M\u0026#39;[\u0026#39;n\u0026#39;]),\rylim = c(0, 20))\rabline(h = log(1/alpha), col = 2)\rabline(v = nu, col = 1)\rif (!is.infinite(n_star)) {\rabline(v = n_star, col = 2)\rprint(paste0(\u0026quot;Change detected at \u0026quot;, n_star))\r} else {\rprint(\u0026quot;No change detected\u0026quot;)\r}\r## [1] \u0026quot;Change detected at 288\u0026quot;\rIn the second example, we started with a fair coin but later switched to a biased coin (\\(q = 0.6\\)) at \\(n = 200\\). Therefore the change point is \\(\\nu = 200\\).\rIn this case, the change detection statistic \\(M_n\\) stays below the detection line \\(y = \\log(1/\\alpha)\\) before the change point \\(n = 200\\) and starts to increase and across the detection line at \\(n = 288\\). Therefore, we are correctly detecting a change with a detection delay \\(N^* - \\nu = 88\\).\nConclusion\rThe Lorden’s lemma tells us that repeated sequential tests can be used as a change detection procedure. Therefore, whenever we can do a sequential test, we can do also a change detection procedure, which is useful to monitor a silent change in the underlying distribution.\nIn many real applications, the pre-change distribution \\(P\\) is often known or can be estimated accurately based on previous sample history under the normal condition. However, the post-change distribution is often unknown. Designing an online change detector that is efficient over a large class of “potential” post-change distributions is still an active research area. See, e.g., arXiv and the reference therein.↩︎\nOriginal Lorden’s lemma has a stronger upper bound on WAD but in this post, I simplified it a bit.↩︎\n","date":1666396800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1666396800,"objectID":"f9e9a7532a8a1a7dc159000be2f4ab8a","permalink":"https://shinjaehyeok.github.io/post/statistics/change_detection/from_seq_test_to_change_detection/","publishdate":"2022-10-22T00:00:00Z","relpermalink":"/post/statistics/change_detection/from_seq_test_to_change_detection/","section":"post","summary":"If we know when a potential change might possibly happen in a stochastic process then we can set a sequential test to continuously monitor whether we can tell the change","tags":["change detection","change point","sequential test","inference"],"title":"From Sequential Tests to Change Detections","type":"post"},{"authors":[],"categories":["Statistics"],"content":"\rIn this follow-up post, we explain another advantage of sequential hypothesis testing: Flexibility and safety. Please visit the previous post for an introduction to Wald’s sequential probability ratio test (SPRT) and a discussion of its sample efficiency.\nCan we early stop the testing if the p-value already reached below \\(\\alpha\\)?\rAgain, let’s start with a simple coin toss example where we observe a sequence of independent observations \\(X_1, X_2, \\dots \\in \\{0,1\\}\\) from a Bernoulli distribution \\(B(p)\\) with \\(p \\in (0,1)\\). We are interested in testing whether the underlying coin is pair (\\(H_0: p = 0.5\\)) or biased toward to head (\\(H_1: p \u0026gt; 0.5\\)).\nFrom the previous post, we know that the binomial test is the best fixed sample size test. Formally, let’s set up the binomial test of level \\(\\alpha = 0.05\\) with the minimum power of \\(\\beta = 0.95\\) at \\(p = 0.6\\). In this case, the minimum sample size is given by the following R script.\nset.seed(1)\rprintf \u0026lt;- function(...) invisible(print(sprintf(...)))\rn_to_power \u0026lt;- function(n, p0, p1, alpha) {\rn \u0026lt;- ceiling(n)\rthres \u0026lt;- qbinom(alpha, n, p0, lower.tail = FALSE) # Reject null if s \u0026gt; thres\rpwr \u0026lt;- pbinom(thres, n, p1, lower.tail = FALSE)\rreturn(pwr)\r}\rpower_to_n \u0026lt;- function(beta, p0, p1, alpha) {\rf \u0026lt;- function(n) {\rbeta - n_to_power(n, p0, p1, alpha)\r}\rroot \u0026lt;- stats::uniroot(f, c(1, 1e+4), tol = 1e-6)\rreturn(ceiling(root$root))\r}\ralpha \u0026lt;- 0.05\rbeta \u0026lt;- 0.95\rp0 \u0026lt;- 0.5\rp1 \u0026lt;- 0.6\rn \u0026lt;- power_to_n(beta, p0 , p1 , alpha)\rprintf(\u0026quot;%i is the minimum sample size to achieve test level %.2f and power %.2f at p= %.1f against the null p=%.2f.\u0026quot;, n, alpha, beta, p1, p0)\r## [1] \u0026quot;280 is the minimum sample size to achieve test level 0.05 and power 0.95 at p= 0.6 against the null p=0.50.\u0026quot;\rIn the standard fixed sample size setting, we compute the p-value once collecting all 280 samples. Then, if the p-value is less than the test level \\(\\alpha\\), we reject the null hypothesis. In this case, we can control the type-1 error below \\(\\alpha\\) and achieve the minimum power \\(\\beta\\) under the alternative (\\(p = 0.6\\)), as the following simulation demonstrates.\nbinom_p_value \u0026lt;- function(s, n, p0) {\rreturn(pbinom(s-1, n, p0, lower.tail = FALSE))\r}\rrun_binom_simul \u0026lt;- function(p_true, n, p0, alpha, max_iter) {\r# We shorten the simulation by using the fact X_1 + ...+X_n ~ Binom(n, p)\rs_vec \u0026lt;- rbinom(max_iter, size = n, prob = p_true)\rp_vec \u0026lt;- sapply(s_vec, binom_p_value, n = n, p0 = p0)\rreturn(mean(p_vec \u0026lt;= alpha))\r}\r# Under the null\rmax_iter \u0026lt;- 1e+4L\rtype_1_err \u0026lt;- run_binom_simul(p_true = p0, n, p0, alpha, max_iter)\rprintf(\u0026quot;Type-1 error of the binomial test is %.2f.\u0026quot;, type_1_err)\r## [1] \u0026quot;Type-1 error of the binomial test is 0.04.\u0026quot;\r# Under the alternative\rpwr \u0026lt;- run_binom_simul(p_true = p1, n, p0, alpha, max_iter)\rprintf(\u0026quot;Power of the binomial test at p=%.1f is %.2f.\u0026quot;, p1, pwr)\r## [1] \u0026quot;Power of the binomial test at p=0.6 is 0.95.\u0026quot;\rWhat if we observe the coin toss one by one and we compute the p-value at each time whenever a new coin toss happens? In the previous post, we noticed that Wald’s sequential probability ratio test (SPRT) can achieve a high sample efficiency by adaptively stopping the test procedure. Why not do the same trick to the binomial test? Can we early stop the testing if the p-value already reached below \\(\\alpha\\)? Let’s run a simulation to check whether this early stopping still results in a type-1 error below the test level \\(\\alpha\\).\nrun_binom_early_stop \u0026lt;- function(p_true, n, p0, alpha) {\rs \u0026lt;- 0\rfor (i in 1:n) {\rs \u0026lt;- s + rbinom(1, size = 1, prob = p_true)\rp \u0026lt;- binom_p_value(s, i, p0)\rif (p \u0026lt;= alpha) {\rreturn(TRUE)\r}\r}\rreturn(FALSE)\r}\r# Under the null\rearly_stopp_type_1_err \u0026lt;- mean(replicate(max_iter,{\rrun_binom_early_stop(p_true = 0.5, n, p0, alpha)\r}))\rprintf(\u0026quot;Type-1 error of the early stopped binomial test is %.2f.\u0026quot;, early_stopp_type_1_err)\r## [1] \u0026quot;Type-1 error of the early stopped binomial test is 0.26.\u0026quot;\rIt turns out that the early stopping strategy increased the type 1 error significantly above the target level. This inflation of type-1 error is an example of p-hacking. This issue is getting worse if we run a longer test.\nset.seed(4)\rs_vec \u0026lt;- cumsum(rbinom(n, size = 1, prob = 0.5))\rf \u0026lt;- function(i) binom_p_value(s_vec[i], i, p0)\rp_vec \u0026lt;- sapply(seq_along(s_vec), f)\rplot(seq_along(p_vec), p_vec, type = \u0026quot;l\u0026quot;, xlab= \u0026quot;n\u0026quot;, ylab =\u0026quot;p-value\u0026quot;)\rabline(h = alpha, col = 2)\rFig. 1 An example sample path of p-values hitting the threshold \\(\\alpha\\) although the underlying samples generated under the null. The red horizontal line corresponds to the test level \\(\\alpha\\). Can we continue to collect more samples if the p-value at the end is slightly above \\(\\alpha\\)?\rOkay, we learned that we should not early stop and wait to collect all samples of the pre-calculated size. But what if the p-value is slightly above \\(\\alpha = 0.05\\) - let’s say \\(p = 0.15\\). Maybe, if we would have collected a bit more samples then the p-value might possibly be less than \\(\\alpha\\) and we could reject the null. Can we continue to collect more samples to see whether the p-value becomes less than \\(\\alpha\\) or goes away from it?\nLet’s run a simple simulation to answer the question. For each run, we first collect the minimum sample size \\(n\\) computed to achieve the minimum power \\(\\beta = 0.95\\). Then, compute the p-value based on the collected \\(n\\) samples. If the p-value is less than \\(0.2\\) then we collect another \\(n\\) samples and re-compute the p-value based on \\(2n\\) samples. Finally, we reject the null if the final p-value is less than or equal to the test level \\(\\alpha\\).\nrun_binom_a_bit_more_simul \u0026lt;- function(p_true, n, p0, alpha, max_iter) {\r# We shorten the simulation by using the fact X_1 + ...+X_n ~ Binom(n, p)\rs_vec \u0026lt;- rbinom(max_iter, size = n, prob = p_true)\rp_vec \u0026lt;- sapply(s_vec, binom_p_value, n = n, p0 = p0)\r# If p-value \u0026gt; alpha and \u0026lt; 0.2, collect n more samples\rabove_alpha_ind \u0026lt;- which(p_vec \u0026gt; alpha \u0026amp; p_vec \u0026lt; 0.2)\rs_vec[above_alpha_ind] \u0026lt;- s_vec[above_alpha_ind] + rbinom(length(above_alpha_ind), size = n, prob = p_true)\rp_vec[above_alpha_ind] \u0026lt;- sapply(s_vec[above_alpha_ind], binom_p_value, n = 2*n, p0 = p0)\rreturn(mean(p_vec \u0026lt;= alpha))\r}\ra_bit_more_type_1_err \u0026lt;- run_binom_a_bit_more_simul(p_true = p0, n, p0, alpha, max_iter)\rprintf(\u0026quot;Type-1 error of the binomial test with the contional continuation is %.3f.\u0026quot;, a_bit_more_type_1_err)\r## [1] \u0026quot;Type-1 error of the binomial test with the contional continuation is 0.063.\u0026quot;\rIn this case, the type-1 error increased above the test level \\(\\alpha\\) though it is not as dramatic as the early stopping case. If we continuously collect and compute p-values rather than collect a single batch then the inflation of the type-1 error becomes more significant. This type of continuation of the testing procedure is also an example of p-hacking.\nrun_binom_a_bit_more2_run \u0026lt;- function(p_true, n, p0, alpha) {\rs \u0026lt;- rbinom(1, size = n, prob = p_true)\rp \u0026lt;- binom_p_value(s, n, p0)\rif (p \u0026lt;= alpha) {\r# If first n samples give p-value less than alpha\r# Reject the null\rreturn(TRUE)\r} else if (p \u0026gt;= 0.2) {\r# If first n samples give p-value above 0.2\r# Fail to reject the null and stop the test\rreturn(FALSE)\r}\r# Otherwise, continue the test until we collect another n samples.\rfor (i in 1:n) {\rs \u0026lt;- s + rbinom(1, size = 1, prob = p_true)\rp \u0026lt;- binom_p_value(s, i, p0)\rif (p \u0026lt;= alpha) {\rreturn(TRUE)\r}\r}\rreturn(FALSE)\r}\r# Under the null\ra_bit_more2_type_1_err \u0026lt;- mean(replicate(max_iter,{\rrun_binom_a_bit_more2_run(p_true = 0.5, n, p0, alpha)\r}))\rprintf(\u0026quot;Type-1 error of the binomial test with the additional continous monitoring is %.2f.\u0026quot;, a_bit_more2_type_1_err)\r## [1] \u0026quot;Type-1 error of the binomial test with the additional continous monitoring is 0.18.\u0026quot;\rAlways-valid p-values - flexibly and safely early stop or continue the test.\rSo far, we have observed how the early stop or continual test results in the type-1 error inflation for fixed sample size tests. The root cause of this inflation is that the p-value from a fixed sample size test is valid only at the pre-specified fixed sample size. In other words, let \\(p_n\\) be the p-value of \\(n\\) samples. Then we have\r\\[\rP_{H_0}\\left(p_n \\leq \\alpha\\right) \\leq \\alpha, ~~\\forall n.\r\\]\rHowever, to early stop or continue the test without the type-1 error inflation, we need the following stronger inequality.\r\\[\rP_{H_0}\\left(\\exists n: p_n \\leq \\alpha\\right) \\leq \\alpha.\r\\]\rNote that fixed sample based p-values do not necessarily satisfy the above strong inequality.1\nIf a random sequence \\(\\{p_n\\}_{n \\geq 1}\\) satisfies the above stronger inequality then it is called “always-valid p-value”. Every sequential hypothesis test has the corresponding always-valid p-value. Therefore, we can flexibly and safely early stop or continue the test.\nAs a concrete example, let’s compute the always-valid p-value of the Wald’s SPRT. Recall that for our Bernoulli example with \\(H_0: p = p_0\\) vs \\(H_1: p = p_1\\), the Wald’s SPRT is a sequential hypothesis testing procedures defined as follows.\r* Set \\(S_0 := 0\\) and for each observation \\(X_n\\), compute the probability ratio \\(\\Lambda_n\\) by\r\\[\r\\Lambda_n := \\left(\\frac{p_1}{p_0}\\right)^{X_n}\\left(\\frac{1-p_1}{1-p_0}\\right)^{1-X_n}.\r\\]\nUpdate \\(S_n := S_{n-1} + \\log \\Lambda_n\\)\nMake one of three following decisions:\nIf \\(S_n \\geq b\\) then stop and reject the null (since there is only one alternative, it is equivalent to accept the alternative).\nIf \\(S_n \\leq a\\) then stop and reject the alternative (or accept the null).\nOtherwise, continue to the next iteration.\nHere, we can set \\(a = \\log(1-\\beta)\\) and \\(b = \\log\\frac{1}{\\alpha}\\). In this case, we can construct an always-valid p-value by\r\\[\rp_n := \\min\\left\\{1, e^{-S_n}\\right\\}.\r\\]\rWe can easily check \\(p_n \\leq \\alpha \\Leftrightarrow S_n \\geq b = \\log(1/\\alpha)\\). Therefore, in terms of the rejection of the null, tracking \\(S_n\\) with the threshold \\(\\log(1/\\alpha)\\) of the Wald’s SPRT is equivalent to tracking \\(p_n\\) with the threshold \\(\\alpha\\). From the type-1 error control of the Wald’s SPRT, we can check \\(p_n\\) is an always-valid p-value satisfying the above stronger inequality.\nset.seed(1)\rsprt_p_value \u0026lt;- function(p_true, p0 = 0.5,\rp1 = 0.6,\rmax_iter = 1e+3L){\rs \u0026lt;- 0\r# Set a placeholder only for visualization.\r# We let the p-value path reaches the maximum to see the convergence\rs_history \u0026lt;- rep(NA, max_iter) p_val_history \u0026lt;- rep(NA, max_iter) for (n in 1:max_iter) {\r# Observe a new sample\rx \u0026lt;- rbinom(1, 1, p_true)\r# Update S_n\rs \u0026lt;- s + ifelse(x == 1, log(p1/p0), log((1-p1)/(1-p0)))\rs_history[n] \u0026lt;- s\rp_val_history[n] \u0026lt;- min(c(1,exp(-s)))\r}\rreturn(list(s = s_history, p_val = p_val_history))\r}\r# Under the null\rnull_out \u0026lt;- sprt_p_value(p_true = 0.5)\rplot(seq_along(null_out$p_val), null_out$p_val, type = \u0026quot;l\u0026quot;,\rxlab = \u0026quot;n\u0026quot;, ylab = \u0026#39;p-value\u0026#39;,\rylim = c(0, 1),\rmain = \u0026quot;Under the null\u0026quot;)\rabline(h = alpha, col = 2)\r# Under the alternative\ralter_out \u0026lt;- sprt_p_value(p_true = 0.6)\rplot(seq_along(alter_out$p_val), alter_out$p_val, type = \u0026quot;l\u0026quot;,\rxlab = \u0026quot;n\u0026quot;, ylab = \u0026#39;p-value\u0026#39;,\rylim = c(0, 1),\rmain = \u0026quot;Under the alternative\u0026quot;)\rabline(h = alpha, col = 2)\rAs a remark, the always-valid p-value above is not uniformly distributed. In fact, we can prove that \\(p_n \\rightarrow 1\\) almost surely under the null and \\(p_n \\rightarrow 0\\) almost surely under the alternative as \\(n \\to \\infty\\). As it is not uniformly distributed, the standard interpretation of the strength of evidence in terms of p-value is not applicable. In fact, though tracking \\(S_n\\) and \\(p_n\\) are equivalent, it is recommended to track \\(S_n\\) directly as the expectation of \\(S_n\\) corresponds to the information gain about the difference between null and alternative distributions.\na \u0026lt;- log(1 - beta)\rb \u0026lt;- log(1 / alpha)\r# Under the null\rplot(seq_along(null_out$s), null_out$s, type = \u0026quot;l\u0026quot;,\rxlab = \u0026quot;n\u0026quot;, ylab = expression(\u0026#39;S\u0026#39;[\u0026#39;n\u0026#39;]),\rmain = \u0026quot;Under the null\u0026quot;)\rabline(h = b, col = 2)\r# Under the alternative\rplot(seq_along(alter_out$s), alter_out$s, type = \u0026quot;l\u0026quot;,\rxlab = \u0026quot;n\u0026quot;, ylab = expression(\u0026#39;S\u0026#39;[\u0026#39;n\u0026#39;]),\rmain = \u0026quot;Under the alternative\u0026quot;)\rabline(h = b, col = 2)\rConclusion\rFixed sample size tests can suffer from type-1 error inflation if the test is not stopped at the pre-specified time. In contrast, sequential hypothesis tests have great flexibility that allows researchers to early stop or continue the experiment without inflating type-1 error.\nIn fact, in most standard fixed sample tests, we have \\(P_{H_0}\\left(\\exists n: p_n \\leq \\alpha\\right) = 1\\).↩︎\n","date":1654387200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1654387200,"objectID":"4138676dbcbfdf6cc69599a590297004","permalink":"https://shinjaehyeok.github.io/post/statistics/sequential_test_safety/sequential_test_flexibility_safety/","publishdate":"2022-06-05T00:00:00Z","relpermalink":"/post/statistics/sequential_test_safety/sequential_test_flexibility_safety/","section":"post","summary":"In this follow-up post, we explain another advantage of sequential hypothesis testing: Flexibility and safety. Please visit the previous post for an introduction to Wald’s","tags":["ab test","inference","sequential test"],"title":"Advantages of Sequential Hypothesis Testing: 2. Flexibility and Safety","type":"post"},{"authors":[],"categories":["Statistics"],"content":"\rIn this and a follow-up posts, we explain two main advantages of sequential hypothesis testing methods compared to standard tests based on fixed sample size.\nSample efficiency in practice\rAs a working example, suppose we have observed a sequence of independent coin tosses, which are encoded by \\(X_n = 1\\) if the \\(n\\)-th observation is head and \\(X_n = 0\\) if it is tail. Statistically, we can model this sequence by independently and identically distributed (i.i.d.) random observations \\(X_1, X_2, \\dots \\in \\{0,1\\}\\) from a Bernoulli distribution \\(B(p)\\) with unknown success probability \\(p \\in (0,1)\\). How can we test whether the coin is fair, i.e., \\(p = p_0:= 0.5\\) or not. For a simple presentation, we assume that if the coin is biased then the biased success probability \\(p_1\\) must be larger than \\(p_0\\).\na) Difficulties in sample size calculation in fixed sample size tests\rIf we know the success probability \\(p\\) must be equal to a constant \\(p_1 \u0026gt; 0.5\\) when the coin turns out to be unfair or biased then the Neyman-Pearson lemma shows that for a fixed test level \\(\\alpha \\in (0,1)\\), the most powerful test is the likelihood-ratio test (LRT) rejecting the null (\\(H_0: p = p_0\\)) in a favor of the alternative (\\(H_1: p = p_1\\)) if \\(\\bar{X}_n \\geq c_\\alpha\\) where \\(\\bar{X}_n := \\sum_{i=1}^n X_i /n\\) and \\(c_\\alpha\\) is a positive constant satisfying \\(P_{H_0}(\\bar{X}_n \\geq c_\\alpha) = \\alpha\\).1\nIn fact, LRT is a uniformly most powerful test against all possible alternatives \\(p_1\\) larger than \\(p_0\\). Hence, if the sample size and test level are fixed, checking whether the sample mean is larger than the critical value \\(c_\\alpha\\) is all you need to do. This test is also called the exact Binomial test.2\nExample R script\nset.seed(1)\rprintf \u0026lt;- function(...) invisible(print(sprintf(...)))\rn \u0026lt;- 100L\r# Testing fair coin sample (alpha = 0.05)\rfair_coin_sample \u0026lt;- rbinom(n, size = 1, prob = 0.5) fair_coin_test_result \u0026lt;- binom.test(sum(fair_coin_sample), n, p = 0.5, alternative = \u0026quot;greater\u0026quot;)\rprintf(\u0026quot;p-value of a fair coin test is %.3f\u0026quot;, fair_coin_test_result$p.value)\r## [1] \u0026quot;p-value of a fair coin test is 0.691\u0026quot;\r# Testing biased coin sample (alpha = 0.05)\rbiased_coin_sample \u0026lt;- rbinom(n, size = 1, prob = 0.6)\rbiased_coin_test_result \u0026lt;- binom.test(sum(biased_coin_sample), n, p = 0.5, alternative = \u0026quot;greater\u0026quot;)\rprintf(\u0026quot;p-value of a biased coin test is %.3f\u0026quot;, biased_coin_test_result$p.value)\r## [1] \u0026quot;p-value of a biased coin test is 0.028\u0026quot;\rHowever, in practice, we rarely know what the alternative success rate could be before an experiment. Instead, we set a minimum effect size of interest \\(\\Delta \u0026gt; 0\\) that we practically consider as a meaningful bias from \\(p_0\\). Based on the minimum effect size \\(\\Delta\\) and desired minimum power \\(\\beta \\in (0,1)\\), we can compute the minimum sample size we need to perform a statistical test that can detect \\(p_1 \u0026gt;= p_0 + \\Delta\\) with probability at least \\(\\beta\\) if the coin is actually biased.\nThe size of \\(\\Delta\\) depends on the application. For example, If we just play a game at a party with friends, we may still consider a coin with a success probability up to \\(0.51\\) as a “fairly fair” coin we can use. In this case, the minimum effect size is \\(\\Delta = 0.1\\). On the other hand, if we play a serious gamble with a large bet on the coin toss then even \\(0.501\\) can be viewed as an unfair coin and the minimum effect size \\(\\Delta\\) is less than \\(0.01\\).\nIn any case, since we do not know the “true” alternative \\(p_1\\), we cannot but set the minimum effect size conservatively. Thus it is possible that the true alternative \\(p_1\\) turns out to be much larger than the boundary \\(p_0 + \\Delta\\). In this case, the minimum sample size we computed can be very larger than what we could have used to detect the alternative at the same level of detection power.\nTo detour this issue, it is common practice to conduct a preliminary study with a small sample size to get a rough estimate of \\(p_1\\) and use it to compute the sample size of the main follow-up study. However, designing a sample efficient preliminary study is itself a non-trivial problem since we cannot reuse samples in the preliminary study for testing the main follow-up experiment.\nb) Sequential tests can choose the sample size adaptively.\rIf there are only two distributions specified in null and alternative hypotheses then similar to the Neyman-Pearson lemma, Wald’s sequential probability ratio test (SPRT) is the test having the smallest average sample size among all statistical tests including both fixed and sequential tests of pre-specified test level \\(\\alpha\\) and minimum power level \\(\\beta\\). It is not contradicting the Neyman-Pearson lemma which only covers all fixed sample size tests. In the sequential test, the sample size is a random stopping time at which we declare whether to reject the null or not.\nFor our Bernoulli example with \\(H_0: p = p_0\\) vs \\(H_1: p = p_1\\), the Wald’s SPRT is given by the following procedure.\nSet \\(S_0 := 0\\) and for each observation \\(X_n\\), compute the probability ratio \\(\\Lambda_n\\) by\r\\[\r\\Lambda_n := \\left(\\frac{p_1}{p_0}\\right)^{X_n}\\left(\\frac{1-p_1}{1-p_0}\\right)^{1-X_n}.\r\\]\nUpdate \\(S_n := S_{n-1} + \\log \\Lambda_n\\)\nMake one of three following decisions:\nIf \\(S_n \\geq b\\) then stop and reject the null (since there is only one alternative, it is equivalent to accept the alternative).\nIf \\(S_n \\leq a\\) then stop and reject the alternative (or accept the null).\nOtherwise, continue to the next iteration.\nHere, thresholds \\(a\\) and \\(b\\) are given approximately \\(a \\sim \\log\\frac{1-\\beta}{1-\\alpha}\\) and \\(b \\sim \\log\\frac{\\beta}{\\alpha}\\) for pre-specified test level \\(\\alpha\\) and minimum power level \\(\\beta\\). Exact thresholds depend on the underlying null and alternative distributions. In the following R example, we use conservative but correct thresholds given by \\(a = \\log(1-\\beta)\\) and \\(b = \\log\\frac{1}{\\alpha}\\).\nExample R script for SPRT under the null\nset.seed(1)\rmax_iter \u0026lt;- 1e+3L\rp0 \u0026lt;- 0.5 # Null distribution\rp1 \u0026lt;- 0.6 # Alternative distribution\ralpha \u0026lt;- 0.05 # Test level\rbeta \u0026lt;- 0.95 # Minimum power level a \u0026lt;- log(1 - beta)\rb \u0026lt;- log(1 / alpha)\rs \u0026lt;- 0\r# Set a placeholder only for visualization.\r# Actual test does not require it. p \u0026lt;- p0 # True distribution is the alternative\rs_history \u0026lt;- rep(NA, max_iter) for (n in 1:max_iter) {\r# Observe a new sample\rx \u0026lt;- rbinom(1, 1, p)\r# Update S_n\rs \u0026lt;- s + ifelse(x == 1, log(p1/p0), log((1-p1)/(1-p0)))\rs_history[n] \u0026lt;- s\r# Make a decision\rif (s \u0026gt;= b) {\rdecision \u0026lt;- \u0026quot;Reject the null\u0026quot;\rbreak } else if (s \u0026lt;= a) {\rdecision \u0026lt;- \u0026quot;Reject the alternative\u0026quot;\rbreak\r} if (n == max_iter) {\rdecision \u0026lt;- \u0026quot;Test reached the max interation\u0026quot;\r}\r}\rprintf(\u0026quot;SPRT was ended at n=%i\u0026quot;, n)\r## [1] \u0026quot;SPRT was ended at n=90\u0026quot;\rprintf(\u0026quot;Decision: %s\u0026quot;, decision)\r## [1] \u0026quot;Decision: Reject the alternative\u0026quot;\rs_history \u0026lt;- s_history[!is.na(s_history)]\rplot(seq_along(s_history), s_history, type = \u0026quot;l\u0026quot;,\rxlab = \u0026quot;n\u0026quot;, ylab = expression(\u0026#39;S\u0026#39;[\u0026#39;n\u0026#39;]),\rylim = c(a, b))\rabline(h = a, col = 2)\rabline(h = b, col = 2)\rFig 1. A sample \\(S_n\\) path of SPRT under the null.\nExample R script for SPRT under the alternative\ns \u0026lt;- 0\r# Set a placeholder only for visualization.\r# Actual test does not require it. p \u0026lt;- p1 # True distribution is the alternative\rs_history \u0026lt;- rep(NA, max_iter) for (n in 1:max_iter) {\r# Observe a new sample\rx \u0026lt;- rbinom(1, 1, p)\r# Update S_n\rs \u0026lt;- s + ifelse(x == 1, log(p1/p0), log((1-p1)/(1-p0)))\rs_history[n] \u0026lt;- s\r# Make a decision\rif (s \u0026gt;= b) {\rdecision \u0026lt;- \u0026quot;Reject the null\u0026quot;\rbreak } else if (s \u0026lt;= a) {\rdecision \u0026lt;- \u0026quot;Reject the alternative\u0026quot;\rbreak\r} if (n == max_iter) {\rdecision \u0026lt;- \u0026quot;Test reached the max interation\u0026quot;\r}\r}\rprintf(\u0026quot;SPRT was ended at n=%i\u0026quot;, n)\r## [1] \u0026quot;SPRT was ended at n=119\u0026quot;\rprintf(\u0026quot;Decision: %s\u0026quot;, decision)\r## [1] \u0026quot;Decision: Reject the null\u0026quot;\rs_history \u0026lt;- s_history[!is.na(s_history)]\rplot(seq_along(s_history), s_history, type = \u0026quot;l\u0026quot;,\rxlab = \u0026quot;n\u0026quot;, ylab = expression(\u0026#39;S\u0026#39;[\u0026#39;n\u0026#39;]),\rylim = c(a, b))\rabline(h = a, col = 2)\rabline(h = b, col = 2)\rFig 2. A sample \\(S_n\\) path of SPRT under the alternative.\nFrom the above example, We can see SPRT ended around \\(90\\sim120\\). However, to achieve the same test level and minimum power, LRT (binomial test in this case), requires at least \\(n = 280\\) samples.3\nSample size calculation for the Binomial test\nn_to_power \u0026lt;- function(n, p0, p1, alpha) {\rn \u0026lt;- ceiling(n)\rthres \u0026lt;- qbinom(alpha, n, p0, lower.tail = FALSE)\rpwr \u0026lt;- pbinom(thres, n, p1, lower.tail = FALSE)\rreturn(pwr)\r}\rpower_to_n \u0026lt;- function(beta, p0, p1, alpha) {\rf \u0026lt;- function(n) {\rbeta - n_to_power(n, p0, p1, alpha)\r}\rroot \u0026lt;- stats::uniroot(f, c(1, 1e+4), tol = 1e-6)\rreturn(ceiling(root$root))\r}\ralpha \u0026lt;- 0.05\rbeta \u0026lt;- 0.95\rp0 \u0026lt;- 0.5\rp1 \u0026lt;- 0.6\rminimum_sample \u0026lt;- power_to_n(beta, p0 , p1 , alpha)\rprintf(\u0026quot;%i is the minimum sample size to achieve test level %.2f and power %.2f.\u0026quot;, minimum_sample, alpha, beta)\r## [1] \u0026quot;280 is the minimum sample size to achieve test level 0.05 and power 0.95.\u0026quot;\rUnlike the LRT, the efficiency of Wald’s SPRT is guaranteed only for a simple alternative hypothesis space in which there is a single alternative distribution. However, SPRT works reasonably well even for misspecified alternative distributions.4\nc) Simulations\rWe end this post by checking the sample efficiency of SPRT over LRT in Bernoulli case simulations. Thoughout the simulation, we use test level \\(\\alpha = 0.05\\), minimum power \\(\\beta = 0.95\\) for hypotheses \\(H_0: p = 0.5\\) vs \\(H_1: p = 0.6\\). In this case, the mimum sample size of LRT (one-sided binomial test) is equal to \\(n = 280\\). Later, we also simulate the misspecified alternatives \\(p = 0.55\\) and \\(p = 0.65\\).\nSet up\n# Set up\ralpha \u0026lt;- 0.05\rp0 \u0026lt;- 0.5\rp1 \u0026lt;- 0.6\rbeta \u0026lt;- 0.95\rn \u0026lt;- power_to_n(beta, p0, p1, alpha) # n = 254\rmax_iter \u0026lt;- 1e+4L\r# One-sided Binomial test run_binom_test \u0026lt;- function(n, p_true,\rp0 = 0.5, p1 = 0.6,\ralpha = 0.05) {\rthres \u0026lt;- qbinom(alpha, n, p0, lower.tail = FALSE)\rnum_head \u0026lt;- rbinom(1, size = n, prob = p_true)\rreturn(num_head \u0026gt; thres)\r}\r# SPRT\rrun_wald_sprt \u0026lt;- function(p_true, p0 = 0.5, p1 = 0.6,\ralpha = 0.05, beta = 0.95, max_iter = 1e+3L) {\ra \u0026lt;- log(1 - beta)\rb \u0026lt;- log(1 / alpha)\rs \u0026lt;- 0\rfor (n in 1:max_iter) {\r# Observe a new sample\rx \u0026lt;- rbinom(1, 1, p_true)\r# Update S_n\rs \u0026lt;- s + ifelse(x == 1, log(p1/p0), log((1-p1)/(1-p0)))\r# Make a decision\rif (s \u0026gt;= b) {\rdecision \u0026lt;- \u0026quot;Reject the null\u0026quot;\ris_reject_null \u0026lt;- TRUE\rbreak } else if (s \u0026lt;= a) {\rdecision \u0026lt;- \u0026quot;Reject the alternative\u0026quot;\ris_reject_null \u0026lt;- FALSE\rbreak\r} if (n == max_iter) {\rdecision \u0026lt;- \u0026quot;Test reached the max interation\u0026quot;\ris_reject_null \u0026lt;- FALSE\r}\r} return(list(\rstopped_n = n,\rdecision = decision,\ris_reject_null = is_reject_null\r))\r}\ri. Under the null (\\(H_0: p = 0.5\\))\rset.seed(1)\r# Under the null p_true = 0.5\r# LRT (binomial)\rbinom_null_err \u0026lt;- mean(replicate(max_iter, {\rrun_binom_test(n, p_true = p0,\rp0, p1,\ralpha)\r}))\rprintf(\u0026quot;Type 1 error of LRT (n = %i) is %.2f\u0026quot;, n, binom_null_err)\r## [1] \u0026quot;Type 1 error of LRT (n = 280) is 0.05\u0026quot;\r# SPRT sprt_null_out \u0026lt;- replicate(max_iter, {\rout \u0026lt;- run_wald_sprt(p_true = p0, p0, p1,\ralpha, beta, max_iter = 1e+3L)\rreturn(c(stopped_n = out$stopped_n, is_reject_null = out$is_reject_null))\r})\rsprt_null_err \u0026lt;- mean(sprt_null_out[\u0026quot;is_reject_null\u0026quot;, ])\rsprt_null_sample_size \u0026lt;- mean(sprt_null_out[\u0026quot;stopped_n\u0026quot;, ])\rprintf(\u0026quot;Type 1 error of SPRT (E[N] = %.1f) is %.2f\u0026quot;, sprt_null_sample_size, sprt_null_err)\r## [1] \u0026quot;Type 1 error of SPRT (E[N] = 137.7) is 0.04\u0026quot;\rUnder the null, both LRT and SPRT control type-1 error, and SPRT has a smaller average sample size compared to the LRT.\nii. Under the alternative (\\(H_1: p = 0.6\\))\rset.seed(1)\r# Under the alternative p_true = 0.6\r# LRT (binomial)\rbinom_power \u0026lt;- mean(replicate(max_iter, {\rrun_binom_test(n, p_true = p1,\rp0, p1,\ralpha)\r}))\rprintf(\u0026quot;Power of LRT (n = %i) is %.2f\u0026quot;, n, binom_power)\r## [1] \u0026quot;Power of LRT (n = 280) is 0.96\u0026quot;\r# SPRT\rsprt_power_out \u0026lt;- replicate(max_iter, {\rout \u0026lt;- run_wald_sprt(p_true = p1, p0, p1,\ralpha, beta, max_iter = 1e+3L)\rreturn(c(stopped_n = out$stopped_n, is_reject_null = out$is_reject_null))\r})\rsprt_power \u0026lt;- mean(sprt_power_out[\u0026quot;is_reject_null\u0026quot;, ])\rsprt_power_sample_size \u0026lt;- mean(sprt_power_out[\u0026quot;stopped_n\u0026quot;, ])\rprintf(\u0026quot;Power of SPRT (E[N] = %.1f) is %.2f\u0026quot;, sprt_power_sample_size, sprt_power)\r## [1] \u0026quot;Power of SPRT (E[N] = 139.3) is 0.96\u0026quot;\rUnder the alternative, both LRT and SPRT achieve the minimum power and SPRT has a smaller average sample size compared to the LRT.\niii. Under a misspecified alternative 1. small \\(p = 0.55\\).\rset.seed(1)\r# Under a misspecified alternative 1. small p_true = 0.55.\rp_mis1 \u0026lt;- 0.55\r# LRT (binomial)\rbinom_power_mis1 \u0026lt;- mean(replicate(max_iter, {\rrun_binom_test(n, p_true = p_mis1,\rp0, p1,\ralpha)\r}))\rprintf(\u0026quot;Power of LRT (n = %i) is %.2f\u0026quot;, n, binom_power_mis1)\r## [1] \u0026quot;Power of LRT (n = 280) is 0.52\u0026quot;\r# SPRT\rsprt_power_mis1_out \u0026lt;- replicate(max_iter, {\rout \u0026lt;- run_wald_sprt(p_true = p_mis1, p0, p1,\ralpha, beta, max_iter = 1e+3L)\rreturn(c(stopped_n = out$stopped_n, is_reject_null = out$is_reject_null))\r})\rsprt_power_mis1 \u0026lt;- mean(sprt_power_mis1_out[\u0026quot;is_reject_null\u0026quot;, ])\rsprt_power_mis_1_sample_size \u0026lt;- mean(sprt_power_mis1_out[\u0026quot;stopped_n\u0026quot;, ])\rprintf(\u0026quot;Power of SPRT (E[N] = %.1f) is %.2f\u0026quot;, sprt_power_mis_1_sample_size, sprt_power_mis1)\r## [1] \u0026quot;Power of SPRT (E[N] = 234.5) is 0.50\u0026quot;\rUnder a misspecified alternative with \\(p = 0.55 \u0026lt; p_1 = 0.6\\), both LRT and SPRT achieve a similar power but SPRT has a smaller average sample size. The achieved power is below the pre-specified minimum power as the true success probability is smaller than the alternative.\niv. Under a misspecified alternative 2. large \\(p = 0.65\\).\rset.seed(1)\r# Under a misspecified alternative 2. large p_true = 0.65.\rp_mis2 \u0026lt;- 0.65\r# LRT (binomial)\rbinom_power_mis2 \u0026lt;- mean(replicate(max_iter, {\rrun_binom_test(n, p_true = p_mis2,\rp0, p1,\ralpha)\r}))\rprintf(\u0026quot;Power of LRT (n = %i) is %.2f\u0026quot;, n, binom_power_mis2)\r## [1] \u0026quot;Power of LRT (n = 280) is 1.00\u0026quot;\rsprt_power_mis2_out \u0026lt;- replicate(max_iter, {\rout \u0026lt;- run_wald_sprt(p_true = p_mis2, p0, p1,\ralpha, beta, max_iter = 1e+3L)\rreturn(c(stopped_n = out$stopped_n, is_reject_null = out$is_reject_null))\r})\rsprt_power_mis2 \u0026lt;- mean(sprt_power_mis2_out[\u0026quot;is_reject_null\u0026quot;, ])\rsprt_power_mis_2_sample_size \u0026lt;- mean(sprt_power_mis2_out[\u0026quot;stopped_n\u0026quot;, ])\rprintf(\u0026quot;Power of SPRT (E[N] = %.1f) is %.2f\u0026quot;, sprt_power_mis_2_sample_size, sprt_power_mis2)\r## [1] \u0026quot;Power of SPRT (E[N] = 76.1) is 1.00\u0026quot;\rUnder a misspecified alternative with \\(p = 0.65 \u0026gt; p_1 = 0.6\\), both LRT and SPRT have almost a power of 1 but SPRT has a much smaller average sample size as SPRT can early stop the procedure adaptively to the underlying higher success probability than the alternative.\nConclusion\rSequential hypothesis testing procedures can achieve a better sample efficiency even compared to the best-fixed sample size test. In the following post, we will explain the flexibility and safety of sequential testing procedures.\nSince \\(\\bar{X}_n\\) is discrete, there might be no such constant \\(c_\\alpha\\) satisfying the equality. In this case, we can randomized the test, which is beyond the scope of this post.↩︎\nComputing the critical value or the corresponding p-value could be computationally expensive if the sample size is large. In this case, we can use a normal approximation-based z-test instead of the exact Binomial test.↩︎\nThis is not a free lunch - in worst scenarios, SPRT can reach the max iteration which could be much larger than the fixed sample size of LRT.↩︎\nWe can use a mixture of SPRTs to achieve a near optimal sample efficiency for a composite alternative hypothesis space in which a range of alternative distributions exist. This topic has been discussed in literature. For example, see arXiv.↩︎\n","date":1654214400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1654214400,"objectID":"7b1f71809dd1f9797e32c713eba1e3cc","permalink":"https://shinjaehyeok.github.io/post/statistics/sequential_test_efficiency/stcd-tutorial/","publishdate":"2022-06-03T00:00:00Z","relpermalink":"/post/statistics/sequential_test_efficiency/stcd-tutorial/","section":"post","summary":"In this and a follow-up posts, we explain two main advantages of sequential hypothesis testing methods compared to standard tests based on fixed sample size. Sample efficiency in practice As","tags":["ab test","inference","sequential test"],"title":"Advantages of Sequential Hypothesis Testing: 1. Sample efficiency","type":"post"}]